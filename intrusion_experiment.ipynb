{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RC9rABjQPeg",
        "outputId": "e882cde6-0fe3-40bb-efa4-42d71d6424f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cuda\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 60.0445\n",
            "Epoch: 0  step: 40  loss: 44.4761\n",
            "Epoch: 0  step: 60  loss: 33.4068\n",
            "Epoch: 0  step: 80  loss: 28.6986\n",
            "Epoch: 0  step: 100  loss: 25.5262\n",
            "Epoch: 0  step: 120  loss: 22.1018\n",
            "Epoch: 0  step: 140  loss: 19.8544\n",
            "Epoch: 0  step: 160  loss: 17.7614\n",
            "Epoch: 0  step: 180  loss: 16.4822\n",
            "Epoch: 0  step: 200  loss: 15.1370\n",
            "Epoch: 0  step: 220  loss: 14.4096\n",
            "Epoch: 0  step: 240  loss: 12.9233\n",
            "Epoch: 0  step: 260  loss: 12.2419\n",
            "Epoch: 0  step: 280  loss: 11.5057\n",
            "Epoch: 0  step: 300  loss: 10.9887\n",
            "Epoch: 0  step: 320  loss: 10.6019\n",
            "Epoch: 0  step: 340  loss: 10.0864\n",
            "Epoch: 0  step: 360  loss: 9.7650\n",
            "Epoch: 0  step: 380  loss: 9.2178\n",
            "Epoch: 0  step: 400  loss: 8.7231\n",
            "Epoch: 0  step: 420  loss: 8.4314\n",
            "Epoch: 0  step: 440  loss: 8.1943\n",
            "Epoch: 0  step: 460  loss: 7.9341\n",
            "Epoch: 0  step: 480  loss: 7.7136\n",
            "Epoch: 0  step: 500  loss: 7.5364\n",
            "Epoch: 0  step: 520  loss: 7.5139\n",
            "Epoch: 0  step: 540  loss: 6.9858\n",
            "Epoch: 0  step: 560  loss: 6.9536\n",
            "Epoch: 0  step: 580  loss: 6.9001\n",
            "Epoch: 0  step: 600  loss: 6.6633\n",
            "Epoch: 0  step: 620  loss: 6.5063\n",
            "Epoch: 0  step: 640  loss: 6.3238\n",
            "Epoch: 0  step: 660  loss: 6.1094\n",
            "Epoch: 0  step: 680  loss: 6.1732\n",
            "Epoch: 0  step: 700  loss: 5.9968\n",
            "Epoch: 0  step: 720  loss: 5.8351\n",
            "Epoch: 0  step: 740  loss: 5.7646\n",
            "Epoch: 0  step: 760  loss: 5.7717\n",
            "Epoch: 0  step: 780  loss: 5.6233\n",
            "Epoch: 0  step: 800  loss: 5.6886\n",
            "Epoch: 0  step: 820  loss: 5.5585\n",
            "Epoch: 0  step: 840  loss: 5.4293\n",
            "Epoch: 0  step: 860  loss: 5.4156\n",
            "Epoch: 0  step: 880  loss: 5.3556\n",
            "Epoch: 0  step: 900  loss: 5.1104\n",
            "Epoch: 0  step: 920  loss: 5.1674\n",
            "Epoch: 0  step: 940  loss: 5.1063\n",
            "Epoch: 0  step: 960  loss: 5.1667\n",
            "Epoch: 0  step: 980  loss: 4.9334\n",
            "Epoch: 0  step: 1000  loss: 4.9155\n",
            "Epoch: 0  step: 1020  loss: 4.8644\n",
            "Epoch: 0  step: 1040  loss: 4.8812\n",
            "Epoch: 0  step: 1060  loss: 4.6312\n",
            "Epoch: 0  step: 1080  loss: 4.6876\n",
            "Epoch: 0  step: 1100  loss: 4.7095\n",
            "Epoch: 0  step: 1120  loss: 4.5474\n",
            "Epoch: 0  step: 1140  loss: 4.5395\n",
            "Epoch: 0  step: 1160  loss: 4.5694\n",
            "Epoch: 0  step: 1180  loss: 4.4755\n",
            "Epoch: 0  step: 1200  loss: 4.3178\n",
            "Epoch: 0  step: 1220  loss: 4.3073\n",
            "Epoch: 0  step: 1240  loss: 4.2663\n",
            "Epoch: 0  step: 1260  loss: 4.2532\n",
            "Epoch: 0  step: 1280  loss: 4.1571\n",
            "Epoch: 0  step: 1300  loss: 4.1076\n",
            "Epoch: 0  step: 1320  loss: 4.1722\n",
            "Epoch: 0  step: 1340  loss: 4.0852\n",
            "Epoch: 0  step: 1360  loss: 4.0225\n",
            "Epoch: 0  step: 1380  loss: 3.9907\n",
            "Epoch: 0  step: 1400  loss: 4.0131\n",
            "Epoch: 0  step: 1420  loss: 4.0008\n",
            "Epoch: 0  step: 1440  loss: 3.9102\n",
            "Epoch: 0  step: 1460  loss: 3.9069\n",
            "Epoch: 0  step: 1480  loss: 3.8792\n",
            "Epoch: 0  step: 1500  loss: 3.8441\n",
            "Epoch: 0  step: 1520  loss: 3.8159\n",
            "Epoch: 0  step: 1540  loss: 3.7924\n",
            "Epoch: 0  step: 1560  loss: 3.7302\n",
            "Epoch: 0  step: 1580  loss: 3.7379\n",
            "Epoch: 0  step: 1600  loss: 3.6844\n",
            "Epoch: 0  step: 1620  loss: 3.6988\n",
            "Epoch: 0  step: 1640  loss: 3.6550\n",
            "Epoch: 0  step: 1660  loss: 3.6516\n",
            "Epoch: 0  step: 1680  loss: 3.6686\n",
            "Epoch: 0  step: 1700  loss: 3.6198\n",
            "Epoch: 0  step: 1720  loss: 3.5869\n",
            "Epoch: 0  step: 1740  loss: 3.5629\n",
            "Epoch: 0  step: 1760  loss: 3.5326\n",
            "Epoch: 0  step: 1780  loss: 3.5297\n",
            "Epoch: 0  step: 1800  loss: 3.5494\n",
            "Epoch: 0  step: 1820  loss: 3.5345\n",
            "Epoch: 0  step: 1840  loss: 3.4785\n",
            "Epoch: 0  step: 1860  loss: 3.5264\n",
            "Epoch: 0  step: 1880  loss: 3.4452\n",
            "Epoch: 0  step: 1900  loss: 3.4117\n",
            "Epoch: 0  step: 1920  loss: 3.4829\n",
            "Epoch: 0  step: 1940  loss: 3.4385\n",
            "Epoch: 0  step: 1960  loss: 3.4063\n",
            "Epoch: 0  step: 1980  loss: 3.4224\n",
            "Epoch: 0  step: 2000  loss: 3.3798\n",
            "Epoch: 0  step: 2020  loss: 3.3603\n",
            "Epoch: 0  step: 2040  loss: 3.3297\n",
            "Epoch: 0  step: 2060  loss: 3.3469\n",
            "Epoch: 0  step: 2080  loss: 3.3586\n",
            "Epoch: 0  step: 2100  loss: 3.3108\n",
            "Epoch: 0  step: 2120  loss: 3.3374\n",
            "Epoch: 0  step: 2140  loss: 3.2790\n",
            "Epoch: 0  step: 2160  loss: 3.3434\n",
            "Epoch: 0  step: 2180  loss: 3.2536\n",
            "Epoch: 0  step: 2200  loss: 3.2547\n",
            "Epoch: 0  step: 2220  loss: 3.2079\n",
            "Epoch: 0  step: 2240  loss: 3.2239\n",
            "Epoch: 0  step: 2260  loss: 3.1933\n",
            "Epoch: 0  step: 2280  loss: 3.2230\n",
            "Epoch: 0  step: 2300  loss: 3.2036\n",
            "Epoch: 0  step: 2320  loss: 3.1766\n",
            "Epoch: 0  step: 2340  loss: 3.1701\n",
            "Epoch: 0  step: 2360  loss: 3.1778\n",
            "Epoch: 0  step: 2380  loss: 3.1485\n",
            "Epoch: 0  step: 2400  loss: 3.1394\n",
            "Epoch: 0  step: 2420  loss: 3.1303\n",
            "Epoch: 0  step: 2440  loss: 3.0882\n",
            "Epoch: 0  step: 2460  loss: 3.1154\n",
            "Epoch: 0  step: 2480  loss: 3.0718\n",
            "Epoch: 0  step: 2500  loss: 3.0739\n",
            "Epoch: 0  step: 2520  loss: 3.0880\n",
            "Epoch: 0  step: 2540  loss: 3.0754\n",
            "Epoch: 0  step: 2560  loss: 3.0700\n",
            "Epoch: 0  step: 2580  loss: 3.0409\n",
            "Epoch: 0  step: 2600  loss: 3.0583\n",
            "Epoch: 0  step: 2620  loss: 3.0278\n",
            "Epoch: 0  step: 2640  loss: 3.0133\n",
            "Epoch: 0  step: 2660  loss: 2.9649\n",
            "Epoch: 0  step: 2680  loss: 2.9452\n",
            "Epoch: 0  step: 2700  loss: 2.9522\n",
            "Epoch: 0  step: 2720  loss: 2.9301\n",
            "Epoch: 0  step: 2740  loss: 2.9428\n",
            "Epoch: 0  step: 2760  loss: 2.8983\n",
            "Epoch: 0  step: 2780  loss: 2.9250\n",
            "Epoch: 0  step: 2800  loss: 2.8882\n",
            "Epoch: 0  step: 2820  loss: 2.8734\n",
            "Epoch: 0  step: 2840  loss: 2.8426\n",
            "Epoch: 0  step: 2860  loss: 2.9314\n",
            "Epoch: 0  step: 2880  loss: 2.8058\n",
            "Epoch: 0  step: 2900  loss: 2.7975\n",
            "Epoch: 0  step: 2920  loss: 2.7895\n",
            "Epoch: 0  step: 2940  loss: 2.7885\n",
            "Epoch: 0  step: 2960  loss: 2.7628\n",
            "Epoch: 0  step: 2980  loss: 2.7485\n",
            "Epoch: 0  step: 3000  loss: 2.7339\n",
            "Epoch: 0  step: 3020  loss: 2.6805\n",
            "Epoch: 0  step: 3040  loss: 2.6881\n",
            "Epoch: 0  step: 3060  loss: 2.7022\n",
            "Epoch: 0  step: 3080  loss: 2.6425\n",
            "Epoch: 0  step: 3100  loss: 2.6748\n",
            "Epoch: 0  step: 3120  loss: 2.6898\n",
            "Epoch: 0  step: 3140  loss: 2.5917\n",
            "Epoch: 0  step: 3160  loss: 2.6232\n",
            "Epoch: 0  step: 3180  loss: 2.5851\n",
            "Epoch: 0  step: 3200  loss: 2.5906\n",
            "Epoch: 0  step: 3220  loss: 2.5571\n",
            "Epoch: 0  step: 3240  loss: 2.5406\n",
            "Epoch: 0  step: 3260  loss: 2.5259\n",
            "Epoch: 0  step: 3280  loss: 2.5082\n",
            "Epoch: 0  step: 3300  loss: 2.4711\n",
            "Epoch: 0  step: 3320  loss: 2.4568\n",
            "Epoch: 0  step: 3340  loss: 2.5277\n",
            "Epoch: 0  step: 3360  loss: 2.4680\n",
            "Epoch: 0  step: 3380  loss: 2.4378\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5DGzSqr7ZdW",
        "outputId": "8145f8ca-3d84-4bbd-fddc-fb1e5642d404"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cuda\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 352.2467\n",
            "Epoch: 0  step: 40  loss: 177.1112\n",
            "Epoch: 0  step: 60  loss: 126.5429\n",
            "Epoch: 0  step: 80  loss: 98.3993\n",
            "Epoch: 0  step: 100  loss: 82.8348\n",
            "Epoch: 0  step: 120  loss: 72.7872\n",
            "Epoch: 0  step: 140  loss: 65.0667\n",
            "Epoch: 0  step: 160  loss: 57.7604\n",
            "Epoch: 0  step: 180  loss: 52.0535\n",
            "Epoch: 0  step: 200  loss: 48.2818\n",
            "Epoch: 0  step: 220  loss: 43.4773\n",
            "Epoch: 0  step: 240  loss: 41.6252\n",
            "Epoch: 0  step: 260  loss: 38.3466\n",
            "Epoch: 0  step: 280  loss: 36.4203\n",
            "Epoch: 0  step: 300  loss: 33.8972\n",
            "Epoch: 0  step: 320  loss: 31.7623\n",
            "Epoch: 0  step: 340  loss: 30.7803\n",
            "Epoch: 0  step: 360  loss: 28.2551\n",
            "Epoch: 0  step: 380  loss: 26.8512\n",
            "Epoch: 1  step: 400  loss: 26.2983\n",
            "Epoch: 1  step: 420  loss: 24.7652\n",
            "Epoch: 1  step: 440  loss: 23.8571\n",
            "Epoch: 1  step: 460  loss: 23.6448\n",
            "Epoch: 1  step: 480  loss: 22.1992\n",
            "Epoch: 1  step: 500  loss: 20.9983\n",
            "Epoch: 1  step: 520  loss: 20.9009\n",
            "Epoch: 1  step: 540  loss: 20.2172\n",
            "Epoch: 1  step: 560  loss: 19.1494\n",
            "Epoch: 1  step: 580  loss: 18.7555\n",
            "Epoch: 1  step: 600  loss: 17.9798\n",
            "Epoch: 1  step: 620  loss: 17.7944\n",
            "Epoch: 1  step: 640  loss: 17.2140\n",
            "Epoch: 1  step: 660  loss: 16.7154\n",
            "Epoch: 1  step: 680  loss: 16.0468\n",
            "Epoch: 1  step: 700  loss: 15.3942\n",
            "Epoch: 1  step: 720  loss: 14.8775\n",
            "Epoch: 1  step: 740  loss: 14.6199\n",
            "Epoch: 1  step: 760  loss: 14.7627\n",
            "Epoch: 1  step: 780  loss: 13.9817\n",
            "Epoch: 2  step: 800  loss: 13.8308\n",
            "Epoch: 2  step: 820  loss: 13.5591\n",
            "Epoch: 2  step: 840  loss: 13.0264\n",
            "Epoch: 2  step: 860  loss: 13.2216\n",
            "Epoch: 2  step: 880  loss: 12.6577\n",
            "Epoch: 2  step: 900  loss: 12.2037\n",
            "Epoch: 2  step: 920  loss: 12.1867\n",
            "Epoch: 2  step: 940  loss: 11.6773\n",
            "Epoch: 2  step: 960  loss: 11.4650\n",
            "Epoch: 2  step: 980  loss: 11.5269\n",
            "Epoch: 2  step: 1000  loss: 11.0710\n",
            "Epoch: 2  step: 1020  loss: 10.8906\n",
            "Epoch: 2  step: 1040  loss: 10.8191\n",
            "Epoch: 2  step: 1060  loss: 10.6029\n",
            "Epoch: 2  step: 1080  loss: 10.5476\n",
            "Epoch: 2  step: 1100  loss: 10.2660\n",
            "Epoch: 2  step: 1120  loss: 9.9882\n",
            "Epoch: 2  step: 1140  loss: 10.0082\n",
            "Epoch: 2  step: 1160  loss: 9.6204\n",
            "Epoch: 3  step: 1180  loss: 9.5369\n",
            "Epoch: 3  step: 1200  loss: 9.6364\n",
            "Epoch: 3  step: 1220  loss: 9.3403\n",
            "Epoch: 3  step: 1240  loss: 9.1719\n",
            "Epoch: 3  step: 1260  loss: 9.0530\n",
            "Epoch: 3  step: 1280  loss: 8.9322\n",
            "Epoch: 3  step: 1300  loss: 8.8662\n",
            "Epoch: 3  step: 1320  loss: 8.7571\n",
            "Epoch: 3  step: 1340  loss: 8.6314\n",
            "Epoch: 3  step: 1360  loss: 8.4195\n",
            "Epoch: 3  step: 1380  loss: 8.4041\n",
            "Epoch: 3  step: 1400  loss: 8.4016\n",
            "Epoch: 3  step: 1420  loss: 8.2640\n",
            "Epoch: 3  step: 1440  loss: 8.0679\n",
            "Epoch: 3  step: 1460  loss: 7.9225\n",
            "Epoch: 3  step: 1480  loss: 7.8648\n",
            "Epoch: 3  step: 1500  loss: 7.7218\n",
            "Epoch: 3  step: 1520  loss: 7.7209\n",
            "Epoch: 3  step: 1540  loss: 7.5009\n",
            "Epoch: 3  step: 1560  loss: 7.5717\n",
            "Epoch: 4  step: 1580  loss: 7.4589\n",
            "Epoch: 4  step: 1600  loss: 7.3675\n",
            "Epoch: 4  step: 1620  loss: 7.3267\n",
            "Epoch: 4  step: 1640  loss: 7.2413\n",
            "Epoch: 4  step: 1660  loss: 7.1021\n",
            "Epoch: 4  step: 1680  loss: 7.0333\n",
            "Epoch: 4  step: 1700  loss: 6.9265\n",
            "Epoch: 4  step: 1720  loss: 6.9244\n",
            "Epoch: 4  step: 1740  loss: 6.8527\n",
            "Epoch: 4  step: 1760  loss: 6.8677\n",
            "Epoch: 4  step: 1780  loss: 6.7793\n",
            "Epoch: 4  step: 1800  loss: 6.7948\n",
            "Epoch: 4  step: 1820  loss: 6.5860\n",
            "Epoch: 4  step: 1840  loss: 6.4309\n",
            "Epoch: 4  step: 1860  loss: 6.5437\n",
            "Epoch: 4  step: 1880  loss: 6.4146\n",
            "Epoch: 4  step: 1900  loss: 6.4903\n",
            "Epoch: 4  step: 1920  loss: 6.3562\n",
            "Epoch: 4  step: 1940  loss: 6.2603\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZBcjbqNioTo",
        "outputId": "30c1843d-65a8-4ebf-a0b3-7e3141a12bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "Normal samples: 2237731, Attack samples: 127693\n",
            "Attack Analysis        → picked 1226 / 1226\n",
            "Attack Backdoor        → picked 2000 / 4659\n",
            "Attack DoS             → picked 2000 / 5980\n",
            "Attack Exploits        → picked 2000 / 42748\n",
            "Attack Fuzzers         → picked 2000 / 33816\n",
            "Attack Generic         → picked 2000 / 19651\n",
            "Attack Reconnaissance  → picked 2000 / 17074\n",
            "Attack Shellcode       → picked 2000 / 2381\n",
            "Attack Worms           → picked 158 / 158\n",
            "Final shapes -> Train: (50000, 51)  Test: (35384, 51)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 34.8306\n",
            "Epoch: 0  step: 40  loss: 23.5891\n",
            "Epoch: 0  step: 60  loss: 16.8174\n",
            "Epoch: 0  step: 80  loss: 13.8163\n",
            "Epoch: 0  step: 100  loss: 11.9837\n",
            "Epoch: 0  step: 120  loss: 10.5595\n",
            "Epoch: 0  step: 140  loss: 9.6095\n",
            "Epoch: 0  step: 160  loss: 8.7865\n",
            "Epoch: 0  step: 180  loss: 8.2394\n",
            "Epoch: 0  step: 200  loss: 7.7003\n",
            "Epoch: 0  step: 220  loss: 7.2694\n",
            "Epoch: 0  step: 240  loss: 6.7324\n",
            "Epoch: 0  step: 260  loss: 6.5456\n",
            "Epoch: 0  step: 280  loss: 5.9491\n",
            "Epoch: 0  step: 300  loss: 5.8146\n",
            "Epoch: 0  step: 320  loss: 5.6311\n",
            "Epoch: 0  step: 340  loss: 5.2757\n",
            "Epoch: 0  step: 360  loss: 4.9684\n",
            "Epoch: 0  step: 380  loss: 4.8306\n",
            "Epoch: 1  step: 400  loss: 4.7574\n",
            "Epoch: 1  step: 420  loss: 4.5693\n",
            "Epoch: 1  step: 440  loss: 4.5010\n",
            "Epoch: 1  step: 460  loss: 4.3199\n",
            "Epoch: 1  step: 480  loss: 4.1113\n",
            "Epoch: 1  step: 500  loss: 4.0234\n",
            "Epoch: 1  step: 520  loss: 3.9647\n",
            "Epoch: 1  step: 540  loss: 3.7813\n",
            "Epoch: 1  step: 560  loss: 3.6184\n",
            "Epoch: 1  step: 580  loss: 3.7019\n",
            "Epoch: 1  step: 600  loss: 3.4965\n",
            "Epoch: 1  step: 620  loss: 3.4232\n",
            "Epoch: 1  step: 640  loss: 3.3582\n",
            "Epoch: 1  step: 660  loss: 3.3099\n",
            "Epoch: 1  step: 680  loss: 3.1775\n",
            "Epoch: 1  step: 700  loss: 3.1808\n",
            "Epoch: 1  step: 720  loss: 3.0500\n",
            "Epoch: 1  step: 740  loss: 2.9535\n",
            "Epoch: 1  step: 760  loss: 2.8936\n",
            "Epoch: 1  step: 780  loss: 2.8950\n",
            "Epoch: 2  step: 800  loss: 2.7725\n",
            "Epoch: 2  step: 820  loss: 2.7900\n",
            "Epoch: 2  step: 840  loss: 2.6576\n",
            "Epoch: 2  step: 860  loss: 2.6377\n",
            "Epoch: 2  step: 880  loss: 2.6134\n",
            "Epoch: 2  step: 900  loss: 2.5203\n",
            "Epoch: 2  step: 920  loss: 2.4578\n",
            "Epoch: 2  step: 940  loss: 2.4494\n",
            "Epoch: 2  step: 960  loss: 2.3879\n",
            "Epoch: 2  step: 980  loss: 2.3144\n",
            "Epoch: 2  step: 1000  loss: 2.2935\n",
            "Epoch: 2  step: 1020  loss: 2.2534\n",
            "Epoch: 2  step: 1040  loss: 2.2693\n",
            "Epoch: 2  step: 1060  loss: 2.1755\n",
            "Epoch: 2  step: 1080  loss: 2.1538\n",
            "Epoch: 2  step: 1100  loss: 2.1583\n",
            "Epoch: 2  step: 1120  loss: 2.1034\n",
            "Epoch: 2  step: 1140  loss: 2.1014\n",
            "Epoch: 2  step: 1160  loss: 2.0416\n",
            "Epoch: 3  step: 1180  loss: 2.0619\n",
            "Epoch: 3  step: 1200  loss: 2.0176\n",
            "Epoch: 3  step: 1220  loss: 1.9810\n",
            "Epoch: 3  step: 1240  loss: 1.9906\n",
            "Epoch: 3  step: 1260  loss: 1.9260\n",
            "Epoch: 3  step: 1280  loss: 1.9026\n",
            "Epoch: 3  step: 1300  loss: 1.9062\n",
            "Epoch: 3  step: 1320  loss: 1.8995\n",
            "Epoch: 3  step: 1340  loss: 1.8807\n",
            "Epoch: 3  step: 1360  loss: 1.8420\n",
            "Epoch: 3  step: 1380  loss: 1.8660\n",
            "Epoch: 3  step: 1400  loss: 1.8279\n",
            "Epoch: 3  step: 1420  loss: 1.8129\n",
            "Epoch: 3  step: 1440  loss: 1.7812\n",
            "Epoch: 3  step: 1460  loss: 1.7460\n",
            "Epoch: 3  step: 1480  loss: 1.7549\n",
            "Epoch: 3  step: 1500  loss: 1.7259\n",
            "Epoch: 3  step: 1520  loss: 1.7469\n",
            "Epoch: 3  step: 1540  loss: 1.7124\n",
            "Epoch: 3  step: 1560  loss: 1.6724\n",
            "Epoch: 4  step: 1580  loss: 1.6573\n",
            "Epoch: 4  step: 1600  loss: 1.6652\n",
            "Epoch: 4  step: 1620  loss: 1.6669\n",
            "Epoch: 4  step: 1640  loss: 1.6197\n",
            "Epoch: 4  step: 1660  loss: 1.6001\n",
            "Epoch: 4  step: 1680  loss: 1.5974\n",
            "Epoch: 4  step: 1700  loss: 1.5771\n",
            "Epoch: 4  step: 1720  loss: 1.5670\n",
            "Epoch: 4  step: 1740  loss: 1.5424\n",
            "Epoch: 4  step: 1760  loss: 1.5388\n",
            "Epoch: 4  step: 1780  loss: 1.5302\n",
            "Epoch: 4  step: 1800  loss: 1.5096\n",
            "Epoch: 4  step: 1820  loss: 1.4959\n",
            "Epoch: 4  step: 1840  loss: 1.4680\n",
            "Epoch: 4  step: 1860  loss: 1.4718\n",
            "Epoch: 4  step: 1880  loss: 1.4604\n",
            "Epoch: 4  step: 1900  loss: 1.4392\n",
            "Epoch: 4  step: 1920  loss: 1.4359\n",
            "Epoch: 4  step: 1940  loss: 1.4304\n",
            "Epoch: 5  step: 1960  loss: 1.4004\n",
            "Epoch: 5  step: 1980  loss: 1.4112\n",
            "Epoch: 5  step: 2000  loss: 1.3816\n",
            "Epoch: 5  step: 2020  loss: 1.3855\n",
            "Epoch: 5  step: 2040  loss: 1.3729\n",
            "Epoch: 5  step: 2060  loss: 1.3663\n",
            "Epoch: 5  step: 2080  loss: 1.3539\n",
            "Epoch: 5  step: 2100  loss: 1.3368\n",
            "Epoch: 5  step: 2120  loss: 1.3286\n",
            "Epoch: 5  step: 2140  loss: 1.3214\n",
            "Epoch: 5  step: 2160  loss: 1.3155\n",
            "Epoch: 5  step: 2180  loss: 1.3117\n",
            "Epoch: 5  step: 2200  loss: 1.3067\n",
            "Epoch: 5  step: 2220  loss: 1.2981\n",
            "Epoch: 5  step: 2240  loss: 1.2734\n",
            "Epoch: 5  step: 2260  loss: 1.2862\n",
            "Epoch: 5  step: 2280  loss: 1.2727\n",
            "Epoch: 5  step: 2300  loss: 1.2796\n",
            "Epoch: 5  step: 2320  loss: 1.2638\n",
            "Epoch: 5  step: 2340  loss: 1.2522\n",
            "Epoch: 6  step: 2360  loss: 1.2461\n",
            "Epoch: 6  step: 2380  loss: 1.2510\n",
            "Epoch: 6  step: 2400  loss: 1.2280\n",
            "Epoch: 6  step: 2420  loss: 1.2399\n",
            "Epoch: 6  step: 2440  loss: 1.2163\n",
            "Epoch: 6  step: 2460  loss: 1.2259\n",
            "Epoch: 6  step: 2480  loss: 1.2093\n",
            "Epoch: 6  step: 2500  loss: 1.2032\n",
            "Epoch: 6  step: 2520  loss: 1.2046\n",
            "Epoch: 6  step: 2540  loss: 1.1933\n",
            "Epoch: 6  step: 2560  loss: 1.1842\n",
            "Epoch: 6  step: 2580  loss: 1.1655\n",
            "Epoch: 6  step: 2600  loss: 1.1688\n",
            "Epoch: 6  step: 2620  loss: 1.1857\n",
            "Epoch: 6  step: 2640  loss: 1.1535\n",
            "Epoch: 6  step: 2660  loss: 1.1608\n",
            "Epoch: 6  step: 2680  loss: 1.1647\n",
            "Epoch: 6  step: 2700  loss: 1.1610\n",
            "Epoch: 6  step: 2720  loss: 1.1402\n",
            "Epoch: 7  step: 2740  loss: 1.1624\n",
            "Epoch: 7  step: 2760  loss: 1.1564\n",
            "Epoch: 7  step: 2780  loss: 1.1534\n",
            "Epoch: 7  step: 2800  loss: 1.1421\n",
            "Epoch: 7  step: 2820  loss: 1.1646\n",
            "Epoch: 7  step: 2840  loss: 1.1559\n",
            "Epoch: 7  step: 2860  loss: 1.1488\n",
            "Epoch: 7  step: 2880  loss: 1.1451\n",
            "Epoch: 7  step: 2900  loss: 1.1221\n",
            "Epoch: 7  step: 2920  loss: 1.1684\n",
            "Epoch: 7  step: 2940  loss: 1.1268\n",
            "Epoch: 7  step: 2960  loss: 1.1359\n",
            "Epoch: 7  step: 2980  loss: 1.1627\n",
            "Epoch: 7  step: 3000  loss: 1.1320\n",
            "Epoch: 7  step: 3020  loss: 1.1139\n",
            "Epoch: 7  step: 3040  loss: 1.1199\n",
            "Epoch: 7  step: 3060  loss: 1.1248\n",
            "Epoch: 7  step: 3080  loss: 1.1356\n",
            "Epoch: 7  step: 3100  loss: 1.1177\n",
            "Epoch: 7  step: 3120  loss: 1.1346\n",
            "Epoch: 8  step: 3140  loss: 1.1548\n",
            "Epoch: 8  step: 3160  loss: 1.1509\n",
            "Epoch: 8  step: 3180  loss: 1.1337\n",
            "Epoch: 8  step: 3200  loss: 1.1286\n",
            "Epoch: 8  step: 3220  loss: 1.1263\n",
            "Epoch: 8  step: 3240  loss: 1.1153\n",
            "Epoch: 8  step: 3260  loss: 1.0982\n",
            "Epoch: 8  step: 3280  loss: 1.0870\n",
            "Epoch: 8  step: 3300  loss: 1.0973\n",
            "Epoch: 8  step: 3320  loss: 1.0917\n",
            "Epoch: 8  step: 3340  loss: 1.0720\n",
            "Epoch: 8  step: 3360  loss: 1.0797\n",
            "Epoch: 8  step: 3380  loss: 1.0736\n",
            "Epoch: 8  step: 3400  loss: 1.0694\n",
            "Epoch: 8  step: 3420  loss: 1.0642\n",
            "Epoch: 8  step: 3440  loss: 1.0843\n",
            "Epoch: 8  step: 3460  loss: 1.0987\n",
            "Epoch: 8  step: 3480  loss: 1.1677\n",
            "Epoch: 8  step: 3500  loss: 1.1254\n",
            "Epoch: 9  step: 3520  loss: 1.1071\n",
            "Epoch: 9  step: 3540  loss: 1.0702\n",
            "Epoch: 9  step: 3560  loss: 1.0755\n",
            "Epoch: 9  step: 3580  loss: 1.0633\n",
            "Epoch: 9  step: 3600  loss: 1.0454\n",
            "Epoch: 9  step: 3620  loss: nan\n",
            "Epoch: 9  step: 3640  loss: nan\n",
            "Epoch: 9  step: 3660  loss: nan\n",
            "Epoch: 9  step: 3680  loss: nan\n",
            "Epoch: 9  step: 3700  loss: nan\n",
            "Epoch: 9  step: 3720  loss: nan\n",
            "Epoch: 9  step: 3740  loss: nan\n",
            "Epoch: 9  step: 3760  loss: nan\n",
            "Epoch: 9  step: 3780  loss: nan\n",
            "Epoch: 9  step: 3800  loss: nan\n",
            "Epoch: 9  step: 3820  loss: nan\n",
            "Epoch: 9  step: 3840  loss: nan\n",
            "Epoch: 9  step: 3860  loss: nan\n",
            "Epoch: 9  step: 3880  loss: nan\n",
            "Epoch: 9  step: 3900  loss: nan\n",
            "Epoch: 10  step: 3920  loss: nan\n",
            "Epoch: 10  step: 3940  loss: nan\n",
            "Epoch: 10  step: 3960  loss: nan\n",
            "Epoch: 10  step: 3980  loss: nan\n",
            "Epoch: 10  step: 4000  loss: nan\n",
            "Epoch: 10  step: 4020  loss: nan\n",
            "Epoch: 10  step: 4040  loss: nan\n",
            "Epoch: 10  step: 4060  loss: nan\n",
            "Epoch: 10  step: 4080  loss: nan\n",
            "Epoch: 10  step: 4100  loss: nan\n",
            "Epoch: 10  step: 4120  loss: nan\n",
            "Epoch: 10  step: 4140  loss: nan\n",
            "Epoch: 10  step: 4160  loss: nan\n",
            "Epoch: 10  step: 4180  loss: nan\n",
            "Epoch: 10  step: 4200  loss: nan\n",
            "Epoch: 10  step: 4220  loss: nan\n",
            "Epoch: 10  step: 4240  loss: nan\n",
            "Epoch: 10  step: 4260  loss: nan\n",
            "Epoch: 10  step: 4280  loss: nan\n",
            "Epoch: 11  step: 4300  loss: nan\n",
            "Epoch: 11  step: 4320  loss: nan\n",
            "Epoch: 11  step: 4340  loss: nan\n",
            "Epoch: 11  step: 4360  loss: nan\n",
            "Epoch: 11  step: 4380  loss: nan\n",
            "Epoch: 11  step: 4400  loss: nan\n",
            "Epoch: 11  step: 4420  loss: nan\n",
            "Epoch: 11  step: 4440  loss: nan\n",
            "Epoch: 11  step: 4460  loss: nan\n",
            "Epoch: 11  step: 4480  loss: nan\n",
            "Epoch: 11  step: 4500  loss: nan\n",
            "Epoch: 11  step: 4520  loss: nan\n",
            "Epoch: 11  step: 4540  loss: nan\n",
            "Epoch: 11  step: 4560  loss: nan\n",
            "Epoch: 11  step: 4580  loss: nan\n",
            "Epoch: 11  step: 4600  loss: nan\n",
            "Epoch: 11  step: 4620  loss: nan\n",
            "Epoch: 11  step: 4640  loss: nan\n",
            "Epoch: 11  step: 4660  loss: nan\n",
            "Epoch: 11  step: 4680  loss: nan\n",
            "Epoch: 12  step: 4700  loss: nan\n",
            "Epoch: 12  step: 4720  loss: nan\n",
            "Epoch: 12  step: 4740  loss: nan\n",
            "Epoch: 12  step: 4760  loss: nan\n",
            "Epoch: 12  step: 4780  loss: nan\n",
            "Epoch: 12  step: 4800  loss: nan\n",
            "Epoch: 12  step: 4820  loss: nan\n",
            "Epoch: 12  step: 4840  loss: nan\n",
            "Epoch: 12  step: 4860  loss: nan\n",
            "Epoch: 12  step: 4880  loss: nan\n",
            "Epoch: 12  step: 4900  loss: nan\n",
            "Epoch: 12  step: 4920  loss: nan\n",
            "Epoch: 12  step: 4940  loss: nan\n",
            "Epoch: 12  step: 4960  loss: nan\n",
            "Epoch: 12  step: 4980  loss: nan\n",
            "Epoch: 12  step: 5000  loss: nan\n",
            "Epoch: 12  step: 5020  loss: nan\n",
            "Epoch: 12  step: 5040  loss: nan\n",
            "Epoch: 12  step: 5060  loss: nan\n",
            "Epoch: 13  step: 5080  loss: nan\n",
            "Epoch: 13  step: 5100  loss: nan\n",
            "Epoch: 13  step: 5120  loss: nan\n",
            "Epoch: 13  step: 5140  loss: nan\n",
            "Epoch: 13  step: 5160  loss: nan\n",
            "Epoch: 13  step: 5180  loss: nan\n",
            "Epoch: 13  step: 5200  loss: nan\n",
            "Epoch: 13  step: 5220  loss: nan\n",
            "Epoch: 13  step: 5240  loss: nan\n",
            "Epoch: 13  step: 5260  loss: nan\n",
            "Epoch: 13  step: 5280  loss: nan\n",
            "Epoch: 13  step: 5300  loss: nan\n",
            "Epoch: 13  step: 5320  loss: nan\n",
            "Epoch: 13  step: 5340  loss: nan\n",
            "Epoch: 13  step: 5360  loss: nan\n",
            "Epoch: 13  step: 5380  loss: nan\n",
            "Epoch: 13  step: 5400  loss: nan\n",
            "Epoch: 13  step: 5420  loss: nan\n",
            "Epoch: 13  step: 5440  loss: nan\n",
            "Epoch: 13  step: 5460  loss: nan\n",
            "Epoch: 14  step: 5480  loss: nan\n",
            "Epoch: 14  step: 5500  loss: nan\n",
            "Epoch: 14  step: 5520  loss: nan\n",
            "Epoch: 14  step: 5540  loss: nan\n",
            "Epoch: 14  step: 5560  loss: nan\n",
            "Epoch: 14  step: 5580  loss: nan\n",
            "Epoch: 14  step: 5600  loss: nan\n",
            "Epoch: 14  step: 5620  loss: nan\n",
            "Epoch: 14  step: 5640  loss: nan\n",
            "Epoch: 14  step: 5660  loss: nan\n",
            "Epoch: 14  step: 5680  loss: nan\n",
            "Epoch: 14  step: 5700  loss: nan\n",
            "Epoch: 14  step: 5720  loss: nan\n",
            "Epoch: 14  step: 5740  loss: nan\n",
            "Epoch: 14  step: 5760  loss: nan\n",
            "Epoch: 14  step: 5780  loss: nan\n",
            "Epoch: 14  step: 5800  loss: nan\n",
            "Epoch: 14  step: 5820  loss: nan\n",
            "Epoch: 14  step: 5840  loss: nan\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2bRO8x_QLRB",
        "outputId": "4ac53835-9140-455d-dfbd-f1940df3fb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal samples: 2237731, Attack samples: 127693\n",
            "Attack Analysis        → picked 1226 / 1226\n",
            "Attack Backdoor        → picked 2000 / 4659\n",
            "Attack DoS             → picked 2000 / 5980\n",
            "Attack Exploits        → picked 2000 / 42748\n",
            "Attack Fuzzers         → picked 2000 / 33816\n",
            "Attack Generic         → picked 2000 / 19651\n",
            "Attack Reconnaissance  → picked 2000 / 17074\n",
            "Attack Shellcode       → picked 2000 / 2381\n",
            "Attack Worms           → picked 158 / 158\n",
            "Final shapes -> Train: (50000, 51)  Test: (35384, 51)\n",
            "threshold:  -4.205\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Accuracy: 0.5652  Precision: 0.0000  Recall: 0.0000  F-score: 0.0000\n"
          ]
        }
      ],
      "source": [
        "! python model_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQprUjC23n1c",
        "outputId": "ac3dfb5b-dcbd-4617-e373-47033f9acdb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "Normal samples: 2237731, Attack samples: 127693\n",
            "Attack Analysis        → picked 1226 / 1226\n",
            "Attack Backdoor        → picked 2000 / 4659\n",
            "Attack DoS             → picked 2000 / 5980\n",
            "Attack Exploits        → picked 2000 / 42748\n",
            "Attack Fuzzers         → picked 2000 / 33816\n",
            "Attack Generic         → picked 2000 / 19651\n",
            "Attack Reconnaissance  → picked 2000 / 17074\n",
            "Attack Shellcode       → picked 2000 / 2381\n",
            "Attack Worms           → picked 158 / 158\n",
            "Final shapes -> Train: (50000, 51)  Test: (35384, 51)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 130.2357\n",
            "Epoch: 0  step: 40  loss: 80.6744\n",
            "Epoch: 0  step: 60  loss: 55.7165\n",
            "Epoch: 0  step: 80  loss: 45.2819\n",
            "Epoch: 0  step: 100  loss: 38.3880\n",
            "Epoch: 0  step: 120  loss: 32.6275\n",
            "Epoch: 0  step: 140  loss: 27.3654\n",
            "Epoch: 0  step: 160  loss: 25.0993\n",
            "Epoch: 0  step: 180  loss: 23.0619\n",
            "Epoch: 0  step: 200  loss: 20.7755\n",
            "Epoch: 0  step: 220  loss: 19.3633\n",
            "Epoch: 0  step: 240  loss: 18.1773\n",
            "Epoch: 0  step: 260  loss: 16.8173\n",
            "Epoch: 0  step: 280  loss: 16.0317\n",
            "Epoch: 0  step: 300  loss: 14.6651\n",
            "Epoch: 0  step: 320  loss: 14.4990\n",
            "Epoch: 0  step: 340  loss: 13.6871\n",
            "Epoch: 0  step: 360  loss: 13.0095\n",
            "Epoch: 0  step: 380  loss: 12.8941\n",
            "Epoch: 1  step: 400  loss: 11.7200\n",
            "Epoch: 1  step: 420  loss: 11.6028\n",
            "Epoch: 1  step: 440  loss: 10.4887\n",
            "Epoch: 1  step: 460  loss: 10.6108\n",
            "Epoch: 1  step: 480  loss: 9.9809\n",
            "Epoch: 1  step: 500  loss: 9.7810\n",
            "Epoch: 1  step: 520  loss: 9.3545\n",
            "Epoch: 1  step: 540  loss: 9.2634\n",
            "Epoch: 1  step: 560  loss: 9.2628\n",
            "Epoch: 1  step: 580  loss: 8.6302\n",
            "Epoch: 1  step: 600  loss: 8.2011\n",
            "Epoch: 1  step: 620  loss: 7.8833\n",
            "Epoch: 1  step: 640  loss: 7.7428\n",
            "Epoch: 1  step: 660  loss: 7.4106\n",
            "Epoch: 1  step: 680  loss: 7.1871\n",
            "Epoch: 1  step: 700  loss: 7.2533\n",
            "Epoch: 1  step: 720  loss: 7.0563\n",
            "Epoch: 1  step: 740  loss: 6.7950\n",
            "Epoch: 1  step: 760  loss: 6.7049\n",
            "Epoch: 1  step: 780  loss: 6.5704\n",
            "Epoch: 2  step: 800  loss: 6.4851\n",
            "Epoch: 2  step: 820  loss: 6.3982\n",
            "Epoch: 2  step: 840  loss: 6.1345\n",
            "Epoch: 2  step: 860  loss: 5.9201\n",
            "Epoch: 2  step: 880  loss: 5.8440\n",
            "Epoch: 2  step: 900  loss: 5.9511\n",
            "Epoch: 2  step: 920  loss: 5.6849\n",
            "Epoch: 2  step: 940  loss: 5.5287\n",
            "Epoch: 2  step: 960  loss: 5.4600\n",
            "Epoch: 2  step: 980  loss: 5.3439\n",
            "Epoch: 2  step: 1000  loss: 5.3612\n",
            "Epoch: 2  step: 1020  loss: 5.2425\n",
            "Epoch: 2  step: 1040  loss: 5.1575\n",
            "Epoch: 2  step: 1060  loss: 4.9355\n",
            "Epoch: 2  step: 1080  loss: 4.9369\n",
            "Epoch: 2  step: 1100  loss: 4.8645\n",
            "Epoch: 2  step: 1120  loss: 4.6008\n",
            "Epoch: 2  step: 1140  loss: 4.6450\n",
            "Epoch: 2  step: 1160  loss: 4.6306\n",
            "Epoch: 3  step: 1180  loss: 4.4468\n",
            "Epoch: 3  step: 1200  loss: 4.5233\n",
            "Epoch: 3  step: 1220  loss: 4.3689\n",
            "Epoch: 3  step: 1240  loss: 4.2856\n",
            "Epoch: 3  step: 1260  loss: 4.1479\n",
            "Epoch: 3  step: 1280  loss: 4.1594\n",
            "Epoch: 3  step: 1300  loss: 4.1010\n",
            "Epoch: 3  step: 1320  loss: 4.0424\n",
            "Epoch: 3  step: 1340  loss: 4.0659\n",
            "Epoch: 3  step: 1360  loss: 3.9925\n",
            "Epoch: 3  step: 1380  loss: 3.9447\n",
            "Epoch: 3  step: 1400  loss: 4.0077\n",
            "Epoch: 3  step: 1420  loss: 3.8975\n",
            "Epoch: 3  step: 1440  loss: 3.8553\n",
            "Epoch: 3  step: 1460  loss: 3.8794\n",
            "Epoch: 3  step: 1480  loss: 3.7575\n",
            "Epoch: 3  step: 1500  loss: 3.7194\n",
            "Epoch: 3  step: 1520  loss: 3.7578\n",
            "Epoch: 3  step: 1540  loss: 3.7009\n",
            "Epoch: 3  step: 1560  loss: 3.6690\n",
            "Epoch: 4  step: 1580  loss: 3.6111\n",
            "Epoch: 4  step: 1600  loss: 3.5438\n",
            "Epoch: 4  step: 1620  loss: 3.5184\n",
            "Epoch: 4  step: 1640  loss: 3.5267\n",
            "Epoch: 4  step: 1660  loss: 3.4571\n",
            "Epoch: 4  step: 1680  loss: 3.5243\n",
            "Epoch: 4  step: 1700  loss: 3.5660\n",
            "Epoch: 4  step: 1720  loss: 3.5078\n",
            "Epoch: 4  step: 1740  loss: 3.3550\n",
            "Epoch: 4  step: 1760  loss: 3.3974\n",
            "Epoch: 4  step: 1780  loss: 3.3468\n",
            "Epoch: 4  step: 1800  loss: 3.3931\n",
            "Epoch: 4  step: 1820  loss: 3.3984\n",
            "Epoch: 4  step: 1840  loss: 3.2725\n",
            "Epoch: 4  step: 1860  loss: 3.3218\n",
            "Epoch: 4  step: 1880  loss: 3.2828\n",
            "Epoch: 4  step: 1900  loss: 3.2277\n",
            "Epoch: 4  step: 1920  loss: 3.2196\n",
            "Epoch: 4  step: 1940  loss: 3.2168\n",
            "Epoch: 5  step: 1960  loss: 3.1834\n",
            "Epoch: 5  step: 1980  loss: 3.1709\n",
            "Epoch: 5  step: 2000  loss: 3.1583\n",
            "Epoch: 5  step: 2020  loss: 3.1752\n",
            "Epoch: 5  step: 2040  loss: 3.1246\n",
            "Epoch: 5  step: 2060  loss: 3.1340\n",
            "Epoch: 5  step: 2080  loss: 3.0489\n",
            "Epoch: 5  step: 2100  loss: 3.0909\n",
            "Epoch: 5  step: 2120  loss: 3.0946\n",
            "Epoch: 5  step: 2140  loss: 3.0941\n",
            "Epoch: 5  step: 2160  loss: 3.0058\n",
            "Epoch: 5  step: 2180  loss: 3.0467\n",
            "Epoch: 5  step: 2200  loss: 3.0458\n",
            "Epoch: 5  step: 2220  loss: 3.0165\n",
            "Epoch: 5  step: 2240  loss: 3.0196\n",
            "Epoch: 5  step: 2260  loss: 2.9670\n",
            "Epoch: 5  step: 2280  loss: 3.0032\n",
            "Epoch: 5  step: 2300  loss: 2.9355\n",
            "Epoch: 5  step: 2320  loss: 2.9760\n",
            "Epoch: 5  step: 2340  loss: 2.9505\n",
            "Epoch: 6  step: 2360  loss: 2.9395\n",
            "Epoch: 6  step: 2380  loss: 2.9002\n",
            "Epoch: 6  step: 2400  loss: 2.8483\n",
            "Epoch: 6  step: 2420  loss: 2.8762\n",
            "Epoch: 6  step: 2440  loss: 2.9025\n",
            "Epoch: 6  step: 2460  loss: 2.8859\n",
            "Epoch: 6  step: 2480  loss: 2.8092\n",
            "Epoch: 6  step: 2500  loss: 2.8175\n",
            "Epoch: 6  step: 2520  loss: 2.8529\n",
            "Epoch: 6  step: 2540  loss: 2.8046\n",
            "Epoch: 6  step: 2560  loss: 2.8345\n",
            "Epoch: 6  step: 2580  loss: 2.7520\n",
            "Epoch: 6  step: 2600  loss: 2.8072\n",
            "Epoch: 6  step: 2620  loss: 2.7762\n",
            "Epoch: 6  step: 2640  loss: 2.7577\n",
            "Epoch: 6  step: 2660  loss: 2.7263\n",
            "Epoch: 6  step: 2680  loss: 2.7644\n",
            "Epoch: 6  step: 2700  loss: 2.7362\n",
            "Epoch: 6  step: 2720  loss: 2.7298\n",
            "Epoch: 7  step: 2740  loss: 2.7038\n",
            "Epoch: 7  step: 2760  loss: 2.7105\n",
            "Epoch: 7  step: 2780  loss: 2.6817\n",
            "Epoch: 7  step: 2800  loss: 2.6926\n",
            "Epoch: 7  step: 2820  loss: 2.6685\n",
            "Epoch: 7  step: 2840  loss: 2.6821\n",
            "Epoch: 7  step: 2860  loss: 2.6688\n",
            "Epoch: 7  step: 2880  loss: 2.6779\n",
            "Epoch: 7  step: 2900  loss: 2.6272\n",
            "Epoch: 7  step: 2920  loss: 2.6551\n",
            "Epoch: 7  step: 2940  loss: 2.6761\n",
            "Epoch: 7  step: 2960  loss: 2.6263\n",
            "Epoch: 7  step: 2980  loss: 2.6065\n",
            "Epoch: 7  step: 3000  loss: 2.6725\n",
            "Epoch: 7  step: 3020  loss: 2.6400\n",
            "Epoch: 7  step: 3040  loss: 2.5956\n",
            "Epoch: 7  step: 3060  loss: 2.5947\n",
            "Epoch: 7  step: 3080  loss: 2.6174\n",
            "Epoch: 7  step: 3100  loss: 2.5316\n",
            "Epoch: 7  step: 3120  loss: 2.5516\n",
            "Epoch: 8  step: 3140  loss: 2.5707\n",
            "Epoch: 8  step: 3160  loss: 2.5704\n",
            "Epoch: 8  step: 3180  loss: 2.5638\n",
            "Epoch: 8  step: 3200  loss: 2.5453\n",
            "Epoch: 8  step: 3220  loss: 2.5505\n",
            "Epoch: 8  step: 3240  loss: 2.5269\n",
            "Epoch: 8  step: 3260  loss: 2.5318\n",
            "Epoch: 8  step: 3280  loss: 2.5272\n",
            "Epoch: 8  step: 3300  loss: 2.4869\n",
            "Epoch: 8  step: 3320  loss: 2.4723\n",
            "Epoch: 8  step: 3340  loss: 2.4991\n",
            "Epoch: 8  step: 3360  loss: 2.5018\n",
            "Epoch: 8  step: 3380  loss: 2.4829\n",
            "Epoch: 8  step: 3400  loss: 2.4943\n",
            "Epoch: 8  step: 3420  loss: 2.4659\n",
            "Epoch: 8  step: 3440  loss: 2.5238\n",
            "Epoch: 8  step: 3460  loss: 2.4870\n",
            "Epoch: 8  step: 3480  loss: 2.4313\n",
            "Epoch: 8  step: 3500  loss: 2.4448\n",
            "Epoch: 9  step: 3520  loss: 2.4503\n",
            "Epoch: 9  step: 3540  loss: 2.4692\n",
            "Epoch: 9  step: 3560  loss: 2.4285\n",
            "Epoch: 9  step: 3580  loss: 2.4345\n",
            "Epoch: 9  step: 3600  loss: 2.4096\n",
            "Epoch: 9  step: 3620  loss: 2.4453\n",
            "Epoch: 9  step: 3640  loss: 2.4477\n",
            "Epoch: 9  step: 3660  loss: 2.4474\n",
            "Epoch: 9  step: 3680  loss: 2.3895\n",
            "Epoch: 9  step: 3700  loss: 2.4183\n",
            "Epoch: 9  step: 3720  loss: 2.4045\n",
            "Epoch: 9  step: 3740  loss: 2.4057\n",
            "Epoch: 9  step: 3760  loss: 2.3676\n",
            "Epoch: 9  step: 3780  loss: 2.3960\n",
            "Epoch: 9  step: 3800  loss: 2.3834\n",
            "Epoch: 9  step: 3820  loss: 2.4273\n",
            "Epoch: 9  step: 3840  loss: 2.3932\n",
            "Epoch: 9  step: 3860  loss: 2.3769\n",
            "Epoch: 9  step: 3880  loss: 2.3590\n",
            "Epoch: 9  step: 3900  loss: 2.3698\n",
            "Epoch: 10  step: 3920  loss: 2.3616\n",
            "Epoch: 10  step: 3940  loss: 2.3600\n",
            "Epoch: 10  step: 3960  loss: 2.3404\n",
            "Epoch: 10  step: 3980  loss: 2.3352\n",
            "Epoch: 10  step: 4000  loss: 2.3418\n",
            "Epoch: 10  step: 4020  loss: 2.3264\n",
            "Epoch: 10  step: 4040  loss: 2.3453\n",
            "Epoch: 10  step: 4060  loss: 2.3704\n",
            "Epoch: 10  step: 4080  loss: 2.3550\n",
            "Epoch: 10  step: 4100  loss: 2.3304\n",
            "Epoch: 10  step: 4120  loss: 2.3376\n",
            "Epoch: 10  step: 4140  loss: 2.3657\n",
            "Epoch: 10  step: 4160  loss: 2.3192\n",
            "Epoch: 10  step: 4180  loss: 2.3324\n",
            "Epoch: 10  step: 4200  loss: 2.3205\n",
            "Epoch: 10  step: 4220  loss: 2.3185\n",
            "Epoch: 10  step: 4240  loss: 2.3190\n",
            "Epoch: 10  step: 4260  loss: 2.2974\n",
            "Epoch: 10  step: 4280  loss: 2.3502\n",
            "Epoch: 11  step: 4300  loss: 2.3376\n",
            "Epoch: 11  step: 4320  loss: 2.3091\n",
            "Epoch: 11  step: 4340  loss: 2.3331\n",
            "Epoch: 11  step: 4360  loss: 2.3211\n",
            "Epoch: 11  step: 4380  loss: 2.3165\n",
            "Epoch: 11  step: 4400  loss: 2.3142\n",
            "Epoch: 11  step: 4420  loss: 2.2905\n",
            "Epoch: 11  step: 4440  loss: 2.2866\n",
            "Epoch: 11  step: 4460  loss: 2.2964\n",
            "Epoch: 11  step: 4480  loss: 2.2804\n",
            "Epoch: 11  step: 4500  loss: 2.2777\n",
            "Epoch: 11  step: 4520  loss: 2.2835\n",
            "Epoch: 11  step: 4540  loss: 2.2766\n",
            "Epoch: 11  step: 4560  loss: 2.2642\n",
            "Epoch: 11  step: 4580  loss: 2.2789\n",
            "Epoch: 11  step: 4600  loss: 2.2757\n",
            "Epoch: 11  step: 4620  loss: 2.2585\n",
            "Epoch: 11  step: 4640  loss: 2.2565\n",
            "Epoch: 11  step: 4660  loss: 2.2731\n",
            "Epoch: 11  step: 4680  loss: 2.2540\n",
            "Epoch: 12  step: 4700  loss: 2.2761\n",
            "Epoch: 12  step: 4720  loss: 2.2530\n",
            "Epoch: 12  step: 4740  loss: 2.2400\n",
            "Epoch: 12  step: 4760  loss: 2.2483\n",
            "Epoch: 12  step: 4780  loss: 2.2651\n",
            "Epoch: 12  step: 4800  loss: 2.2945\n",
            "Epoch: 12  step: 4820  loss: 2.2464\n",
            "Epoch: 12  step: 4840  loss: 2.2645\n",
            "Epoch: 12  step: 4860  loss: 2.2557\n",
            "Epoch: 12  step: 4880  loss: 2.2452\n",
            "Epoch: 12  step: 4900  loss: 2.2703\n",
            "Epoch: 12  step: 4920  loss: 2.2592\n",
            "Epoch: 12  step: 4940  loss: 2.2451\n",
            "Epoch: 12  step: 4960  loss: 2.2283\n",
            "Epoch: 12  step: 4980  loss: 2.2478\n",
            "Epoch: 12  step: 5000  loss: 2.2472\n",
            "Epoch: 12  step: 5020  loss: 2.2376\n",
            "Epoch: 12  step: 5040  loss: 2.2266\n",
            "Epoch: 12  step: 5060  loss: 2.2140\n",
            "Epoch: 13  step: 5080  loss: 2.2108\n",
            "Epoch: 13  step: 5100  loss: 2.2415\n",
            "Epoch: 13  step: 5120  loss: 2.2402\n",
            "Epoch: 13  step: 5140  loss: 2.2280\n",
            "Epoch: 13  step: 5160  loss: 2.2284\n",
            "Epoch: 13  step: 5180  loss: 2.2211\n",
            "Epoch: 13  step: 5200  loss: 2.2178\n",
            "Epoch: 13  step: 5220  loss: 2.2164\n",
            "Epoch: 13  step: 5240  loss: 2.2411\n",
            "Epoch: 13  step: 5260  loss: 2.2296\n",
            "Epoch: 13  step: 5280  loss: 2.2029\n",
            "Epoch: 13  step: 5300  loss: 2.2094\n",
            "Epoch: 13  step: 5320  loss: 2.2041\n",
            "Epoch: 13  step: 5340  loss: 2.2083\n",
            "Epoch: 13  step: 5360  loss: 2.1961\n",
            "Epoch: 13  step: 5380  loss: 2.2197\n",
            "Epoch: 13  step: 5400  loss: 2.2196\n",
            "Epoch: 13  step: 5420  loss: 2.1988\n",
            "Epoch: 13  step: 5440  loss: 2.2280\n",
            "Epoch: 13  step: 5460  loss: 2.2447\n",
            "Epoch: 14  step: 5480  loss: 2.1913\n",
            "Epoch: 14  step: 5500  loss: 2.2133\n",
            "Epoch: 14  step: 5520  loss: 2.2111\n",
            "Epoch: 14  step: 5540  loss: 2.2276\n",
            "Epoch: 14  step: 5560  loss: 2.2081\n",
            "Epoch: 14  step: 5580  loss: 2.2021\n",
            "Epoch: 14  step: 5600  loss: 2.2330\n",
            "Epoch: 14  step: 5620  loss: 2.1777\n",
            "Epoch: 14  step: 5640  loss: 2.2117\n",
            "Epoch: 14  step: 5660  loss: 2.1974\n",
            "Epoch: 14  step: 5680  loss: 2.2112\n",
            "Epoch: 14  step: 5700  loss: 2.1934\n",
            "Epoch: 14  step: 5720  loss: 2.1951\n",
            "Epoch: 14  step: 5740  loss: 2.2038\n",
            "Epoch: 14  step: 5760  loss: 2.2064\n",
            "Epoch: 14  step: 5780  loss: 2.1882\n",
            "Epoch: 14  step: 5800  loss: 2.1956\n",
            "Epoch: 14  step: 5820  loss: 2.1988\n",
            "Epoch: 14  step: 5840  loss: 2.2025\n",
            "Epoch: 15  step: 5860  loss: 2.1978\n",
            "Epoch: 15  step: 5880  loss: 2.1796\n",
            "Epoch: 15  step: 5900  loss: 2.1821\n",
            "Epoch: 15  step: 5920  loss: 2.2110\n",
            "Epoch: 15  step: 5940  loss: 2.1867\n",
            "Epoch: 15  step: 5960  loss: 2.2045\n",
            "Epoch: 15  step: 5980  loss: 2.1932\n",
            "Epoch: 15  step: 6000  loss: 2.1892\n",
            "Epoch: 15  step: 6020  loss: 2.1894\n",
            "Epoch: 15  step: 6040  loss: 2.2073\n",
            "Epoch: 15  step: 6060  loss: 2.1931\n",
            "Epoch: 15  step: 6080  loss: 2.1960\n",
            "Epoch: 15  step: 6100  loss: 2.1838\n",
            "Epoch: 15  step: 6120  loss: 2.2006\n",
            "Epoch: 15  step: 6140  loss: 2.2217\n",
            "Epoch: 15  step: 6160  loss: 2.1746\n",
            "Epoch: 15  step: 6180  loss: 2.1795\n",
            "Epoch: 15  step: 6200  loss: 2.1765\n",
            "Epoch: 15  step: 6220  loss: 2.1839\n",
            "Epoch: 15  step: 6240  loss: 2.1752\n",
            "Epoch: 16  step: 6260  loss: 2.1964\n",
            "Epoch: 16  step: 6280  loss: 2.1796\n",
            "Epoch: 16  step: 6300  loss: 2.1763\n",
            "Epoch: 16  step: 6320  loss: 2.1836\n",
            "Epoch: 16  step: 6340  loss: 2.1776\n",
            "Epoch: 16  step: 6360  loss: 2.1830\n",
            "Epoch: 16  step: 6380  loss: 2.1797\n",
            "Epoch: 16  step: 6400  loss: 2.1853\n",
            "Epoch: 16  step: 6420  loss: 2.1783\n",
            "Epoch: 16  step: 6440  loss: 2.1697\n",
            "Epoch: 16  step: 6460  loss: 2.2250\n",
            "Epoch: 16  step: 6480  loss: 2.1620\n",
            "Epoch: 16  step: 6500  loss: 2.1645\n",
            "Epoch: 16  step: 6520  loss: 2.2015\n",
            "Epoch: 16  step: 6540  loss: 2.1739\n",
            "Epoch: 16  step: 6560  loss: 2.1996\n",
            "Epoch: 16  step: 6580  loss: 2.1659\n",
            "Epoch: 16  step: 6600  loss: 2.1852\n",
            "Epoch: 16  step: 6620  loss: 2.1631\n",
            "Epoch: 17  step: 6640  loss: 2.1987\n",
            "Epoch: 17  step: 6660  loss: 2.1562\n",
            "Epoch: 17  step: 6680  loss: 2.1791\n",
            "Epoch: 17  step: 6700  loss: 2.1813\n",
            "Epoch: 17  step: 6720  loss: 2.1801\n",
            "Epoch: 17  step: 6740  loss: 2.1620\n",
            "Epoch: 17  step: 6760  loss: 2.1779\n",
            "Epoch: 17  step: 6780  loss: 2.1782\n",
            "Epoch: 17  step: 6800  loss: 2.1632\n",
            "Epoch: 17  step: 6820  loss: 2.1791\n",
            "Epoch: 17  step: 6840  loss: 2.1922\n",
            "Epoch: 17  step: 6860  loss: 2.1967\n",
            "Epoch: 17  step: 6880  loss: 2.1802\n",
            "Epoch: 17  step: 6900  loss: 2.1817\n",
            "Epoch: 17  step: 6920  loss: 2.1866\n",
            "Epoch: 17  step: 6940  loss: 2.1685\n",
            "Epoch: 17  step: 6960  loss: 2.1849\n",
            "Epoch: 17  step: 6980  loss: 2.2017\n",
            "Epoch: 17  step: 7000  loss: 2.1515\n",
            "Epoch: 17  step: 7020  loss: 2.1668\n",
            "Epoch: 18  step: 7040  loss: 2.1948\n",
            "Epoch: 18  step: 7060  loss: 2.1703\n",
            "Epoch: 18  step: 7080  loss: 2.1821\n",
            "Epoch: 18  step: 7100  loss: 2.1557\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/model_train.py\", line 96, in <module>\n",
            "    main()\n",
            "  File \"/content/model_train.py\", line 76, in main\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 647, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 829, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6cJABR-u8I-",
        "outputId": "7065bb66-8427-4fef-ba71-2b423383ddd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "Normal samples: 2237731, Attack samples: 127693\n",
            "Attack Analysis        → picked 1226 / 1226\n",
            "Attack Backdoor        → picked 2000 / 4659\n",
            "Attack DoS             → picked 2000 / 5980\n",
            "Attack Exploits        → picked 2000 / 42748\n",
            "Attack Fuzzers         → picked 2000 / 33816\n",
            "Attack Generic         → picked 2000 / 19651\n",
            "Attack Reconnaissance  → picked 2000 / 17074\n",
            "Attack Shellcode       → picked 2000 / 2381\n",
            "Attack Worms           → picked 158 / 158\n",
            "Final shapes -> Train: (50000, 51)  Test: (35384, 51)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 6.3054\n",
            "Epoch: 0  step: 40  loss: 5.8171\n",
            "Epoch: 0  step: 60  loss: 5.3176\n",
            "Epoch: 0  step: 80  loss: 4.8821\n",
            "Epoch: 0  step: 100  loss: 4.4552\n",
            "Epoch: 0  step: 120  loss: 4.0935\n",
            "Epoch: 0  step: 140  loss: 3.7712\n",
            "Epoch: 0  step: 160  loss: 3.5640\n",
            "Epoch: 0  step: 180  loss: 3.4431\n",
            "Epoch: 0  step: 200  loss: 3.3496\n",
            "Epoch: 0  step: 220  loss: 3.2627\n",
            "Epoch: 0  step: 240  loss: 3.2276\n",
            "Epoch: 0  step: 260  loss: 3.1672\n",
            "Epoch: 0  step: 280  loss: 3.1200\n",
            "Epoch: 0  step: 300  loss: 3.0831\n",
            "Epoch: 0  step: 320  loss: 3.0720\n",
            "Epoch: 0  step: 340  loss: 3.0588\n",
            "Epoch: 0  step: 360  loss: 3.0065\n",
            "Epoch: 0  step: 380  loss: 3.0280\n",
            "Epoch: 1  step: 400  loss: 2.9896\n",
            "Epoch: 1  step: 420  loss: 2.9581\n",
            "Epoch: 1  step: 440  loss: 2.9628\n",
            "Epoch: 1  step: 460  loss: 2.9324\n",
            "Epoch: 1  step: 480  loss: 2.9270\n",
            "Epoch: 1  step: 500  loss: 2.9326\n",
            "Epoch: 1  step: 520  loss: 2.9052\n",
            "Epoch: 1  step: 540  loss: 2.8963\n",
            "Epoch: 1  step: 560  loss: 2.8940\n",
            "Epoch: 1  step: 580  loss: 2.8363\n",
            "Epoch: 1  step: 600  loss: 2.8442\n",
            "Epoch: 1  step: 620  loss: 2.8270\n",
            "Epoch: 1  step: 640  loss: 2.7996\n",
            "Epoch: 1  step: 660  loss: 2.7851\n",
            "Epoch: 1  step: 680  loss: 2.7689\n",
            "Epoch: 1  step: 700  loss: 2.7830\n",
            "Epoch: 1  step: 720  loss: 2.7641\n",
            "Epoch: 1  step: 740  loss: 2.7535\n",
            "Epoch: 1  step: 760  loss: 2.7179\n",
            "Epoch: 1  step: 780  loss: 2.7266\n",
            "Epoch: 2  step: 800  loss: 2.6897\n",
            "Epoch: 2  step: 820  loss: 2.6607\n",
            "Epoch: 2  step: 840  loss: 2.6384\n",
            "Epoch: 2  step: 860  loss: 2.6780\n",
            "Epoch: 2  step: 880  loss: 2.6245\n",
            "Epoch: 2  step: 900  loss: 2.6338\n",
            "Epoch: 2  step: 920  loss: 2.6034\n",
            "Epoch: 2  step: 940  loss: 2.5955\n",
            "Epoch: 2  step: 960  loss: 2.5920\n",
            "Epoch: 2  step: 980  loss: 2.5725\n",
            "Epoch: 2  step: 1000  loss: 2.5933\n",
            "Epoch: 2  step: 1020  loss: 2.5743\n",
            "Epoch: 2  step: 1040  loss: 2.5469\n",
            "Epoch: 2  step: 1060  loss: 2.5376\n",
            "Epoch: 2  step: 1080  loss: 2.5038\n",
            "Epoch: 2  step: 1100  loss: 2.5451\n",
            "Epoch: 2  step: 1120  loss: 2.5285\n",
            "Epoch: 2  step: 1140  loss: 2.5142\n",
            "Epoch: 2  step: 1160  loss: 2.5033\n",
            "Epoch: 3  step: 1180  loss: 2.5014\n",
            "Epoch: 3  step: 1200  loss: 2.4762\n",
            "Epoch: 3  step: 1220  loss: 2.4635\n",
            "Epoch: 3  step: 1240  loss: 2.4698\n",
            "Epoch: 3  step: 1260  loss: 2.4525\n",
            "Epoch: 3  step: 1280  loss: 2.4331\n",
            "Epoch: 3  step: 1300  loss: 2.4523\n",
            "Epoch: 3  step: 1320  loss: 2.4547\n",
            "Epoch: 3  step: 1340  loss: 2.4258\n",
            "Epoch: 3  step: 1360  loss: 2.4314\n",
            "Epoch: 3  step: 1380  loss: 2.4188\n",
            "Epoch: 3  step: 1400  loss: 2.4110\n",
            "Epoch: 3  step: 1420  loss: 2.4162\n",
            "Epoch: 3  step: 1440  loss: 2.4186\n",
            "Epoch: 3  step: 1460  loss: 2.3856\n",
            "Epoch: 3  step: 1480  loss: 2.3581\n",
            "Epoch: 3  step: 1500  loss: 2.3639\n",
            "Epoch: 3  step: 1520  loss: 2.3726\n",
            "Epoch: 3  step: 1540  loss: 2.3699\n",
            "Epoch: 3  step: 1560  loss: 2.3718\n",
            "Epoch: 4  step: 1580  loss: 2.3618\n",
            "Epoch: 4  step: 1600  loss: 2.3645\n",
            "Epoch: 4  step: 1620  loss: 2.3620\n",
            "Epoch: 4  step: 1640  loss: 2.3488\n",
            "Epoch: 4  step: 1660  loss: 2.3442\n",
            "Epoch: 4  step: 1680  loss: 2.3377\n",
            "Epoch: 4  step: 1700  loss: 2.3475\n",
            "Epoch: 4  step: 1720  loss: 2.3374\n",
            "Epoch: 4  step: 1740  loss: 2.2895\n",
            "Epoch: 4  step: 1760  loss: 2.3232\n",
            "Epoch: 4  step: 1780  loss: 2.3052\n",
            "Epoch: 4  step: 1800  loss: 2.3061\n",
            "Epoch: 4  step: 1820  loss: 2.3082\n",
            "Epoch: 4  step: 1840  loss: 2.2938\n",
            "Epoch: 4  step: 1860  loss: 2.2882\n",
            "Epoch: 4  step: 1880  loss: 2.2726\n",
            "Epoch: 4  step: 1900  loss: 2.2832\n",
            "Epoch: 4  step: 1920  loss: 2.2698\n",
            "Epoch: 4  step: 1940  loss: 2.2610\n",
            "Epoch: 5  step: 1960  loss: 2.2573\n",
            "Epoch: 5  step: 1980  loss: 2.2604\n",
            "Epoch: 5  step: 2000  loss: 2.2648\n",
            "Epoch: 5  step: 2020  loss: 2.2332\n",
            "Epoch: 5  step: 2040  loss: 2.2268\n",
            "Epoch: 5  step: 2060  loss: 2.2302\n",
            "Epoch: 5  step: 2080  loss: 2.2256\n",
            "Epoch: 5  step: 2100  loss: 2.2305\n",
            "Epoch: 5  step: 2120  loss: 2.2271\n",
            "Epoch: 5  step: 2140  loss: 2.2454\n",
            "Epoch: 5  step: 2160  loss: 2.2155\n",
            "Epoch: 5  step: 2180  loss: 2.2085\n",
            "Epoch: 5  step: 2200  loss: 2.2132\n",
            "Epoch: 5  step: 2220  loss: 2.2372\n",
            "Epoch: 5  step: 2240  loss: 2.2092\n",
            "Epoch: 5  step: 2260  loss: 2.2162\n",
            "Epoch: 5  step: 2280  loss: 2.2105\n",
            "Epoch: 5  step: 2300  loss: 2.2093\n",
            "Epoch: 5  step: 2320  loss: 2.2147\n",
            "Epoch: 5  step: 2340  loss: 2.2101\n",
            "Epoch: 6  step: 2360  loss: 2.1765\n",
            "Epoch: 6  step: 2380  loss: 2.1869\n",
            "Epoch: 6  step: 2400  loss: 2.1863\n",
            "Epoch: 6  step: 2420  loss: 2.1873\n",
            "Epoch: 6  step: 2440  loss: 2.1851\n",
            "Epoch: 6  step: 2460  loss: 2.1810\n",
            "Epoch: 6  step: 2480  loss: 2.1556\n",
            "Epoch: 6  step: 2500  loss: 2.1678\n",
            "Epoch: 6  step: 2520  loss: 2.1733\n",
            "Epoch: 6  step: 2540  loss: 2.1773\n",
            "Epoch: 6  step: 2560  loss: 2.1849\n",
            "Epoch: 6  step: 2580  loss: 2.1518\n",
            "Epoch: 6  step: 2600  loss: 2.1575\n",
            "Epoch: 6  step: 2620  loss: 2.1430\n",
            "Epoch: 6  step: 2640  loss: 2.1357\n",
            "Epoch: 6  step: 2660  loss: 2.1410\n",
            "Epoch: 6  step: 2680  loss: 2.1378\n",
            "Epoch: 6  step: 2700  loss: 2.1391\n",
            "Epoch: 6  step: 2720  loss: 2.1548\n",
            "Epoch: 7  step: 2740  loss: 2.1313\n",
            "Epoch: 7  step: 2760  loss: 2.1312\n",
            "Epoch: 7  step: 2780  loss: 2.1417\n",
            "Epoch: 7  step: 2800  loss: 2.1192\n",
            "Epoch: 7  step: 2820  loss: 2.1372\n",
            "Epoch: 7  step: 2840  loss: 2.1089\n",
            "Epoch: 7  step: 2860  loss: 2.1210\n",
            "Epoch: 7  step: 2880  loss: 2.1260\n",
            "Epoch: 7  step: 2900  loss: 2.1094\n",
            "Epoch: 7  step: 2920  loss: 2.1000\n",
            "Epoch: 7  step: 2940  loss: 2.1337\n",
            "Epoch: 7  step: 2960  loss: 2.1106\n",
            "Epoch: 7  step: 2980  loss: 2.1153\n",
            "Epoch: 7  step: 3000  loss: 2.1174\n",
            "Epoch: 7  step: 3020  loss: 2.0810\n",
            "Epoch: 7  step: 3040  loss: 2.0968\n",
            "Epoch: 7  step: 3060  loss: 2.0989\n",
            "Epoch: 7  step: 3080  loss: 2.1101\n",
            "Epoch: 7  step: 3100  loss: 2.0955\n",
            "Epoch: 7  step: 3120  loss: 2.0994\n",
            "Epoch: 8  step: 3140  loss: 2.0777\n",
            "Epoch: 8  step: 3160  loss: 2.0887\n",
            "Epoch: 8  step: 3180  loss: 2.0767\n",
            "Epoch: 8  step: 3200  loss: 2.0743\n",
            "Epoch: 8  step: 3220  loss: 2.1129\n",
            "Epoch: 8  step: 3240  loss: 2.1019\n",
            "Epoch: 8  step: 3260  loss: 2.0728\n",
            "Epoch: 8  step: 3280  loss: 2.0887\n",
            "Epoch: 8  step: 3300  loss: 2.0699\n",
            "Epoch: 8  step: 3320  loss: 2.0669\n",
            "Epoch: 8  step: 3340  loss: 2.0731\n",
            "Epoch: 8  step: 3360  loss: 2.0503\n",
            "Epoch: 8  step: 3380  loss: 2.0680\n",
            "Epoch: 8  step: 3400  loss: 2.0697\n",
            "Epoch: 8  step: 3420  loss: 2.0427\n",
            "Epoch: 8  step: 3440  loss: 2.0738\n",
            "Epoch: 8  step: 3460  loss: 2.0585\n",
            "Epoch: 8  step: 3480  loss: 2.0718\n",
            "Epoch: 8  step: 3500  loss: 2.0825\n",
            "Epoch: 9  step: 3520  loss: 2.0471\n",
            "Epoch: 9  step: 3540  loss: 2.0814\n",
            "Epoch: 9  step: 3560  loss: 2.0608\n",
            "Epoch: 9  step: 3580  loss: 2.0562\n",
            "Epoch: 9  step: 3600  loss: 2.0442\n",
            "Epoch: 9  step: 3620  loss: 2.0620\n",
            "Epoch: 9  step: 3640  loss: 2.0470\n",
            "Epoch: 9  step: 3660  loss: 2.0588\n",
            "Epoch: 9  step: 3680  loss: 2.0417\n",
            "Epoch: 9  step: 3700  loss: 2.0842\n",
            "Epoch: 9  step: 3720  loss: 2.0397\n",
            "Epoch: 9  step: 3740  loss: 2.0542\n",
            "Epoch: 9  step: 3760  loss: 2.0107\n",
            "Epoch: 9  step: 3780  loss: 2.0487\n",
            "Epoch: 9  step: 3800  loss: 2.0290\n",
            "Epoch: 9  step: 3820  loss: 2.0574\n",
            "Epoch: 9  step: 3840  loss: 2.0482\n",
            "Epoch: 9  step: 3860  loss: 2.0671\n",
            "Epoch: 9  step: 3880  loss: 2.0348\n",
            "Epoch: 9  step: 3900  loss: 2.0506\n",
            "Epoch: 10  step: 3920  loss: 2.0252\n",
            "Epoch: 10  step: 3940  loss: 2.0304\n",
            "Epoch: 10  step: 3960  loss: 2.0150\n",
            "Epoch: 10  step: 3980  loss: 2.0334\n",
            "Epoch: 10  step: 4000  loss: 2.0448\n",
            "Epoch: 10  step: 4020  loss: 2.0406\n",
            "Epoch: 10  step: 4040  loss: 2.0360\n",
            "Epoch: 10  step: 4060  loss: 2.0263\n",
            "Epoch: 10  step: 4080  loss: 2.0350\n",
            "Epoch: 10  step: 4100  loss: 2.0066\n",
            "Epoch: 10  step: 4120  loss: 2.0218\n",
            "Epoch: 10  step: 4140  loss: 2.0143\n",
            "Epoch: 10  step: 4160  loss: 2.0207\n",
            "Epoch: 10  step: 4180  loss: 2.0189\n",
            "Epoch: 10  step: 4200  loss: 2.0076\n",
            "Epoch: 10  step: 4220  loss: 1.9966\n",
            "Epoch: 10  step: 4240  loss: 1.9819\n",
            "Epoch: 10  step: 4260  loss: 2.0332\n",
            "Epoch: 10  step: 4280  loss: 1.9916\n",
            "Epoch: 11  step: 4300  loss: 2.0381\n",
            "Epoch: 11  step: 4320  loss: 2.0055\n",
            "Epoch: 11  step: 4340  loss: 2.0091\n",
            "Epoch: 11  step: 4360  loss: 2.0138\n",
            "Epoch: 11  step: 4380  loss: 2.0122\n",
            "Epoch: 11  step: 4400  loss: 2.0133\n",
            "Epoch: 11  step: 4420  loss: 2.0059\n",
            "Epoch: 11  step: 4440  loss: 2.0082\n",
            "Epoch: 11  step: 4460  loss: 1.9991\n",
            "Epoch: 11  step: 4480  loss: 2.0022\n",
            "Epoch: 11  step: 4500  loss: 2.0070\n",
            "Epoch: 11  step: 4520  loss: 1.9946\n",
            "Epoch: 11  step: 4540  loss: 2.0136\n",
            "Epoch: 11  step: 4560  loss: 2.0098\n",
            "Epoch: 11  step: 4580  loss: 1.9970\n",
            "Epoch: 11  step: 4600  loss: 2.0322\n",
            "Epoch: 11  step: 4620  loss: 1.9970\n",
            "Epoch: 11  step: 4640  loss: 1.9918\n",
            "Epoch: 11  step: 4660  loss: 1.9881\n",
            "Epoch: 11  step: 4680  loss: 1.9692\n",
            "Epoch: 12  step: 4700  loss: 2.0069\n",
            "Epoch: 12  step: 4720  loss: 1.9651\n",
            "Epoch: 12  step: 4740  loss: 1.9835\n",
            "Epoch: 12  step: 4760  loss: 2.0003\n",
            "Epoch: 12  step: 4780  loss: 2.0219\n",
            "Epoch: 12  step: 4800  loss: 1.9774\n",
            "Epoch: 12  step: 4820  loss: 1.9978\n",
            "Epoch: 12  step: 4840  loss: 1.9705\n",
            "Epoch: 12  step: 4860  loss: 2.0000\n",
            "Epoch: 12  step: 4880  loss: 1.9901\n",
            "Epoch: 12  step: 4900  loss: 1.9796\n",
            "Epoch: 12  step: 4920  loss: 1.9860\n",
            "Epoch: 12  step: 4940  loss: 1.9822\n",
            "Epoch: 12  step: 4960  loss: 1.9809\n",
            "Epoch: 12  step: 4980  loss: 1.9661\n",
            "Epoch: 12  step: 5000  loss: 1.9965\n",
            "Epoch: 12  step: 5020  loss: 1.9683\n",
            "Epoch: 12  step: 5040  loss: 1.9861\n",
            "Epoch: 12  step: 5060  loss: 1.9782\n",
            "Epoch: 13  step: 5080  loss: 1.9853\n",
            "Epoch: 13  step: 5100  loss: 1.9773\n",
            "Epoch: 13  step: 5120  loss: 1.9707\n",
            "Epoch: 13  step: 5140  loss: 2.0003\n",
            "Epoch: 13  step: 5160  loss: 2.0028\n",
            "Epoch: 13  step: 5180  loss: 1.9904\n",
            "Epoch: 13  step: 5200  loss: 1.9807\n",
            "Epoch: 13  step: 5220  loss: 2.0021\n",
            "Epoch: 13  step: 5240  loss: 2.0033\n",
            "Epoch: 13  step: 5260  loss: 1.9777\n",
            "Epoch: 13  step: 5280  loss: 1.9631\n",
            "Epoch: 13  step: 5300  loss: 1.9973\n",
            "Epoch: 13  step: 5320  loss: 1.9468\n",
            "Epoch: 13  step: 5340  loss: 1.9489\n",
            "Epoch: 13  step: 5360  loss: 2.0030\n",
            "Epoch: 13  step: 5380  loss: 1.9760\n",
            "Epoch: 13  step: 5400  loss: 1.9525\n",
            "Epoch: 13  step: 5420  loss: 1.9635\n",
            "Epoch: 13  step: 5440  loss: 1.9874\n",
            "Epoch: 13  step: 5460  loss: 1.9797\n",
            "Epoch: 14  step: 5480  loss: 2.0069\n",
            "Epoch: 14  step: 5500  loss: 1.9510\n",
            "Epoch: 14  step: 5520  loss: 1.9628\n",
            "Epoch: 14  step: 5540  loss: 2.0015\n",
            "Epoch: 14  step: 5560  loss: 1.9483\n",
            "Epoch: 14  step: 5580  loss: 1.9365\n",
            "Epoch: 14  step: 5600  loss: 1.9738\n",
            "Epoch: 14  step: 5620  loss: 1.9815\n",
            "Epoch: 14  step: 5640  loss: 1.9760\n",
            "Epoch: 14  step: 5660  loss: 1.9756\n",
            "Epoch: 14  step: 5680  loss: 1.9791\n",
            "Epoch: 14  step: 5700  loss: 1.9908\n",
            "Epoch: 14  step: 5720  loss: 1.9801\n",
            "Epoch: 14  step: 5740  loss: 1.9639\n",
            "Epoch: 14  step: 5760  loss: 1.9344\n",
            "Epoch: 14  step: 5780  loss: 1.9795\n",
            "Epoch: 14  step: 5800  loss: 1.9760\n",
            "Epoch: 14  step: 5820  loss: 1.9512\n",
            "Epoch: 14  step: 5840  loss: 1.9652\n",
            "Epoch: 15  step: 5860  loss: 1.9684\n",
            "Epoch: 15  step: 5880  loss: 1.9690\n",
            "Epoch: 15  step: 5900  loss: 1.9819\n",
            "Epoch: 15  step: 5920  loss: 1.9607\n",
            "Epoch: 15  step: 5940  loss: 1.9802\n",
            "Epoch: 15  step: 5960  loss: 1.9646\n",
            "Epoch: 15  step: 5980  loss: 2.0030\n",
            "Epoch: 15  step: 6000  loss: 1.9785\n",
            "Epoch: 15  step: 6020  loss: 1.9612\n",
            "Epoch: 15  step: 6040  loss: 1.9778\n",
            "Epoch: 15  step: 6060  loss: 1.9526\n",
            "Epoch: 15  step: 6080  loss: 1.9378\n",
            "Epoch: 15  step: 6100  loss: 1.9821\n",
            "Epoch: 15  step: 6120  loss: 1.9488\n",
            "Epoch: 15  step: 6140  loss: 1.9640\n",
            "Epoch: 15  step: 6160  loss: 1.9674\n",
            "Epoch: 15  step: 6180  loss: 1.9836\n",
            "Epoch: 15  step: 6200  loss: 1.9544\n",
            "Epoch: 15  step: 6220  loss: 1.9732\n",
            "Epoch: 15  step: 6240  loss: 1.9589\n",
            "Epoch: 16  step: 6260  loss: 2.0010\n",
            "Epoch: 16  step: 6280  loss: 1.9662\n",
            "Epoch: 16  step: 6300  loss: 1.9547\n",
            "Epoch: 16  step: 6320  loss: 1.9784\n",
            "Epoch: 16  step: 6340  loss: 1.9440\n",
            "Epoch: 16  step: 6360  loss: 1.9748\n",
            "Epoch: 16  step: 6380  loss: 1.9741\n",
            "Epoch: 16  step: 6400  loss: 1.9641\n",
            "Epoch: 16  step: 6420  loss: 1.9573\n",
            "Epoch: 16  step: 6440  loss: 1.9480\n",
            "Epoch: 16  step: 6460  loss: 1.9532\n",
            "Epoch: 16  step: 6480  loss: 1.9702\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/model_train.py\", line 96, in <module>\n",
            "    main()\n",
            "  File \"/content/model_train.py\", line 69, in main\n",
            "    gamma = estimator(z)\n",
            "            ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1769, in _wrapped_call_impl\n",
            "    def _wrapped_call_impl(self, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkpvXjDfNCqV",
        "outputId": "2e2757f6-f4e9-4a32-ed09-5701e655e0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on: cpu\n",
            "Normal samples: 1550712, Attack samples: 72406\n",
            "Final shapes -> Train: (1000000, 10)  Test: (623118, 10)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Epoch: 0  step: 20  loss: 2.4725\n",
            "Epoch: 0  step: 40  loss: 2.3727\n",
            "Epoch: 0  step: 60  loss: 2.2738\n",
            "Epoch: 0  step: 80  loss: 2.1865\n",
            "Epoch: 0  step: 100  loss: 2.1012\n",
            "Epoch: 0  step: 120  loss: 2.0217\n",
            "Epoch: 0  step: 140  loss: 1.9508\n",
            "Epoch: 0  step: 160  loss: 1.8781\n",
            "Epoch: 0  step: 180  loss: 1.8044\n",
            "Epoch: 0  step: 200  loss: 1.7429\n",
            "Epoch: 0  step: 220  loss: 1.6845\n",
            "Epoch: 0  step: 240  loss: 1.6148\n",
            "Epoch: 0  step: 260  loss: 1.5579\n",
            "Epoch: 0  step: 280  loss: 1.4838\n",
            "Epoch: 0  step: 300  loss: 1.4390\n",
            "Epoch: 0  step: 320  loss: 1.3847\n",
            "Epoch: 0  step: 340  loss: 1.3388\n",
            "Epoch: 0  step: 360  loss: 1.2796\n",
            "Epoch: 0  step: 380  loss: 1.2346\n",
            "Epoch: 0  step: 400  loss: 1.1822\n",
            "Epoch: 0  step: 420  loss: 1.1318\n",
            "Epoch: 0  step: 440  loss: 1.0853\n",
            "Epoch: 0  step: 460  loss: 1.0529\n",
            "Epoch: 0  step: 480  loss: 1.0202\n",
            "Epoch: 0  step: 500  loss: 0.9862\n",
            "Epoch: 0  step: 520  loss: 0.9518\n",
            "Epoch: 0  step: 540  loss: 0.9176\n",
            "Epoch: 0  step: 560  loss: 0.8936\n",
            "Epoch: 0  step: 580  loss: 0.8750\n",
            "Epoch: 0  step: 600  loss: 0.8527\n",
            "Epoch: 0  step: 620  loss: 0.8276\n",
            "Epoch: 0  step: 640  loss: 0.8039\n",
            "Epoch: 0  step: 660  loss: 0.7843\n",
            "Epoch: 0  step: 680  loss: 0.8013\n",
            "Epoch: 0  step: 700  loss: 0.7720\n",
            "Epoch: 0  step: 720  loss: 0.7472\n",
            "Epoch: 0  step: 740  loss: 0.7610\n",
            "Epoch: 0  step: 760  loss: 0.7431\n",
            "Epoch: 0  step: 780  loss: 0.7251\n",
            "Epoch: 0  step: 800  loss: 0.7201\n",
            "Epoch: 0  step: 820  loss: 0.7241\n",
            "Epoch: 0  step: 840  loss: 0.7037\n",
            "Epoch: 0  step: 860  loss: 0.6870\n",
            "Epoch: 0  step: 880  loss: 0.6987\n",
            "Epoch: 0  step: 900  loss: 0.6671\n",
            "Epoch: 0  step: 920  loss: 0.6770\n",
            "Epoch: 0  step: 940  loss: 0.6457\n",
            "Epoch: 0  step: 960  loss: 0.6663\n",
            "Epoch: 0  step: 980  loss: 0.6497\n",
            "Epoch: 0  step: 1000  loss: 0.6479\n",
            "Epoch: 0  step: 1020  loss: 0.6546\n",
            "Epoch: 0  step: 1040  loss: 0.6362\n",
            "Epoch: 0  step: 1060  loss: 0.6246\n",
            "Epoch: 0  step: 1080  loss: 0.6188\n",
            "Epoch: 0  step: 1100  loss: 0.6026\n",
            "Epoch: 0  step: 1120  loss: 0.6164\n",
            "Epoch: 0  step: 1140  loss: 0.6025\n",
            "Epoch: 0  step: 1160  loss: 0.6044\n",
            "Epoch: 0  step: 1180  loss: 0.6010\n",
            "Epoch: 0  step: 1200  loss: 0.6105\n",
            "Epoch: 0  step: 1220  loss: 0.5897\n",
            "Epoch: 0  step: 1240  loss: 0.5832\n",
            "Epoch: 0  step: 1260  loss: 0.5818\n",
            "Epoch: 0  step: 1280  loss: 0.5848\n",
            "Epoch: 0  step: 1300  loss: 0.5753\n",
            "Epoch: 0  step: 1320  loss: 0.5773\n",
            "Epoch: 0  step: 1340  loss: 0.5731\n",
            "Epoch: 0  step: 1360  loss: 0.5690\n",
            "Epoch: 0  step: 1380  loss: 0.5652\n",
            "Epoch: 0  step: 1400  loss: 0.5649\n",
            "Epoch: 0  step: 1420  loss: 0.5511\n",
            "Epoch: 0  step: 1440  loss: 0.5456\n",
            "Epoch: 0  step: 1460  loss: 0.5478\n",
            "Epoch: 0  step: 1480  loss: 0.5331\n",
            "Epoch: 0  step: 1500  loss: 0.5314\n",
            "Epoch: 0  step: 1520  loss: 0.5413\n",
            "Epoch: 0  step: 1540  loss: 0.5272\n",
            "Epoch: 0  step: 1560  loss: 0.5180\n",
            "Epoch: 0  step: 1580  loss: 0.5306\n",
            "Epoch: 0  step: 1600  loss: 0.5161\n",
            "Epoch: 0  step: 1620  loss: 0.5075\n",
            "Epoch: 0  step: 1640  loss: 0.5184\n",
            "Epoch: 0  step: 1660  loss: 0.5053\n",
            "Epoch: 0  step: 1680  loss: 0.5088\n",
            "Epoch: 0  step: 1700  loss: 0.5121\n",
            "Epoch: 0  step: 1720  loss: 0.4967\n",
            "Epoch: 0  step: 1740  loss: 0.5032\n",
            "Epoch: 0  step: 1760  loss: 0.5001\n",
            "Epoch: 0  step: 1780  loss: 0.4904\n",
            "Epoch: 0  step: 1800  loss: 0.4779\n",
            "Epoch: 0  step: 1820  loss: 0.4940\n",
            "Epoch: 0  step: 1840  loss: 0.4936\n",
            "Epoch: 0  step: 1860  loss: 0.4859\n",
            "Epoch: 0  step: 1880  loss: 0.4919\n",
            "Epoch: 0  step: 1900  loss: 0.4854\n",
            "Epoch: 0  step: 1920  loss: 0.4644\n",
            "Epoch: 0  step: 1940  loss: 0.4706\n",
            "Epoch: 0  step: 1960  loss: 0.4808\n",
            "Epoch: 0  step: 1980  loss: 0.4765\n",
            "Epoch: 0  step: 2000  loss: 0.4627\n",
            "Epoch: 0  step: 2020  loss: 0.4636\n",
            "Epoch: 0  step: 2040  loss: 0.4531\n",
            "Epoch: 0  step: 2060  loss: 0.4619\n",
            "Epoch: 0  step: 2080  loss: 0.4524\n",
            "Epoch: 0  step: 2100  loss: 0.4653\n",
            "Epoch: 0  step: 2120  loss: 0.4542\n",
            "Epoch: 0  step: 2140  loss: 0.4570\n",
            "Epoch: 0  step: 2160  loss: 0.4581\n",
            "Epoch: 0  step: 2180  loss: 0.4669\n",
            "Epoch: 0  step: 2200  loss: 0.4453\n",
            "Epoch: 0  step: 2220  loss: 0.4459\n",
            "Epoch: 0  step: 2240  loss: 0.4519\n",
            "Epoch: 0  step: 2260  loss: 0.4531\n",
            "Epoch: 0  step: 2280  loss: 0.4447\n",
            "Epoch: 0  step: 2300  loss: 0.4525\n",
            "Epoch: 0  step: 2320  loss: 0.4365\n",
            "Epoch: 0  step: 2340  loss: 0.4397\n",
            "Epoch: 0  step: 2360  loss: 0.4440\n",
            "Epoch: 0  step: 2380  loss: 0.4443\n",
            "Epoch: 0  step: 2400  loss: 0.4223\n",
            "Epoch: 0  step: 2420  loss: 0.4320\n",
            "Epoch: 0  step: 2440  loss: 0.4410\n",
            "Epoch: 0  step: 2460  loss: 0.4419\n",
            "Epoch: 0  step: 2480  loss: 0.4396\n",
            "Epoch: 0  step: 2500  loss: 0.4307\n",
            "Epoch: 0  step: 2520  loss: 0.4196\n",
            "Epoch: 0  step: 2540  loss: 0.4275\n",
            "Epoch: 0  step: 2560  loss: 0.4378\n",
            "Epoch: 0  step: 2580  loss: 0.4228\n",
            "Epoch: 0  step: 2600  loss: 0.4218\n",
            "Epoch: 0  step: 2620  loss: 0.4201\n",
            "Epoch: 0  step: 2640  loss: 0.4233\n",
            "Epoch: 0  step: 2660  loss: 0.4204\n",
            "Epoch: 0  step: 2680  loss: 0.4290\n",
            "Epoch: 0  step: 2700  loss: 0.4325\n",
            "Epoch: 0  step: 2720  loss: 0.4169\n",
            "Epoch: 0  step: 2740  loss: 0.4146\n",
            "Epoch: 0  step: 2760  loss: 0.4239\n",
            "Epoch: 0  step: 2780  loss: 0.4257\n",
            "Epoch: 0  step: 2800  loss: 0.4215\n",
            "Epoch: 0  step: 2820  loss: 0.4102\n",
            "Epoch: 0  step: 2840  loss: 0.4153\n",
            "Epoch: 0  step: 2860  loss: 0.4101\n",
            "Epoch: 0  step: 2880  loss: 0.4053\n",
            "Epoch: 0  step: 2900  loss: 0.4166\n",
            "Epoch: 0  step: 2920  loss: 0.4031\n",
            "Epoch: 0  step: 2940  loss: 0.4125\n",
            "Epoch: 0  step: 2960  loss: 0.4072\n",
            "Epoch: 0  step: 2980  loss: 0.4060\n",
            "Epoch: 0  step: 3000  loss: 0.4147\n",
            "Epoch: 0  step: 3020  loss: 0.4057\n",
            "Epoch: 0  step: 3040  loss: 0.4042\n",
            "Epoch: 0  step: 3060  loss: 0.4065\n",
            "Epoch: 0  step: 3080  loss: 0.4076\n",
            "Epoch: 0  step: 3100  loss: 0.3963\n",
            "Epoch: 0  step: 3120  loss: 0.4025\n",
            "Epoch: 0  step: 3140  loss: 0.4046\n",
            "Epoch: 0  step: 3160  loss: 0.4116\n",
            "Epoch: 0  step: 3180  loss: 0.3996\n",
            "Epoch: 0  step: 3200  loss: 0.4004\n",
            "Epoch: 0  step: 3220  loss: 0.4010\n",
            "Epoch: 0  step: 3240  loss: 0.4092\n",
            "Epoch: 0  step: 3260  loss: 0.3873\n",
            "Epoch: 0  step: 3280  loss: 0.3982\n",
            "Epoch: 0  step: 3300  loss: 0.3913\n",
            "Epoch: 0  step: 3320  loss: 0.3869\n",
            "Epoch: 0  step: 3340  loss: 0.3904\n",
            "Epoch: 0  step: 3360  loss: 0.3947\n",
            "Epoch: 0  step: 3380  loss: 0.3887\n",
            "Epoch: 0  step: 3400  loss: 0.3756\n",
            "Epoch: 0  step: 3420  loss: 0.3920\n",
            "Epoch: 0  step: 3440  loss: 0.3880\n",
            "Epoch: 0  step: 3460  loss: 0.3888\n",
            "Epoch: 0  step: 3480  loss: 0.3930\n",
            "Epoch: 0  step: 3500  loss: 0.3889\n",
            "Epoch: 0  step: 3520  loss: 0.3915\n",
            "Epoch: 0  step: 3540  loss: 0.3894\n",
            "Epoch: 0  step: 3560  loss: 0.3890\n",
            "Epoch: 0  step: 3580  loss: 0.3857\n",
            "Epoch: 0  step: 3600  loss: 0.3723\n",
            "Epoch: 0  step: 3620  loss: 0.3734\n",
            "Epoch: 0  step: 3640  loss: 0.3963\n",
            "Epoch: 0  step: 3660  loss: 0.3799\n",
            "Epoch: 0  step: 3680  loss: 0.3813\n",
            "Epoch: 0  step: 3700  loss: 0.3875\n",
            "Epoch: 0  step: 3720  loss: 0.3790\n",
            "Epoch: 0  step: 3740  loss: 0.3721\n",
            "Epoch: 0  step: 3760  loss: 0.3861\n",
            "Epoch: 0  step: 3780  loss: 0.3805\n",
            "Epoch: 0  step: 3800  loss: 0.3935\n",
            "Epoch: 0  step: 3820  loss: 0.3807\n",
            "Epoch: 0  step: 3840  loss: 0.3634\n",
            "Epoch: 0  step: 3860  loss: 0.3712\n",
            "Epoch: 0  step: 3880  loss: 0.3748\n",
            "Epoch: 0  step: 3900  loss: 0.3802\n",
            "Epoch: 0  step: 3920  loss: 0.3803\n",
            "Epoch: 0  step: 3940  loss: 0.3773\n",
            "Epoch: 0  step: 3960  loss: 0.3637\n",
            "Epoch: 0  step: 3980  loss: 0.3706\n",
            "Epoch: 0  step: 4000  loss: 0.3762\n",
            "Epoch: 0  step: 4020  loss: 0.3615\n",
            "Epoch: 0  step: 4040  loss: 0.3686\n",
            "Epoch: 0  step: 4060  loss: 0.3650\n",
            "Epoch: 0  step: 4080  loss: 0.3767\n",
            "Epoch: 0  step: 4100  loss: 0.3689\n",
            "Epoch: 0  step: 4120  loss: 0.3687\n",
            "Epoch: 0  step: 4140  loss: 0.3642\n",
            "Epoch: 0  step: 4160  loss: 0.3654\n",
            "Epoch: 0  step: 4180  loss: 0.3722\n",
            "Epoch: 0  step: 4200  loss: 0.3650\n",
            "Epoch: 0  step: 4220  loss: 0.3602\n",
            "Epoch: 0  step: 4240  loss: 0.3598\n",
            "Epoch: 0  step: 4260  loss: 0.3668\n",
            "Epoch: 0  step: 4280  loss: 0.3580\n",
            "Epoch: 0  step: 4300  loss: 0.3596\n",
            "Epoch: 0  step: 4320  loss: 0.3549\n",
            "Epoch: 0  step: 4340  loss: 0.3660\n",
            "Epoch: 0  step: 4360  loss: 0.3589\n",
            "Epoch: 0  step: 4380  loss: 0.3689\n",
            "Epoch: 0  step: 4400  loss: 0.3651\n",
            "Epoch: 0  step: 4420  loss: 0.3586\n",
            "Epoch: 0  step: 4440  loss: 0.3664\n",
            "Epoch: 0  step: 4460  loss: 0.3466\n",
            "Epoch: 0  step: 4480  loss: 0.3549\n",
            "Epoch: 0  step: 4500  loss: 0.3523\n",
            "Epoch: 0  step: 4520  loss: 0.3589\n",
            "Epoch: 0  step: 4540  loss: 0.3543\n",
            "Epoch: 0  step: 4560  loss: 0.3575\n",
            "Epoch: 0  step: 4580  loss: 0.3568\n",
            "Epoch: 0  step: 4600  loss: 0.3575\n",
            "Epoch: 0  step: 4620  loss: 0.3541\n",
            "Epoch: 0  step: 4640  loss: 0.3636\n",
            "Epoch: 0  step: 4660  loss: 0.3438\n",
            "Epoch: 0  step: 4680  loss: 0.3413\n",
            "Epoch: 0  step: 4700  loss: 0.3451\n",
            "Epoch: 0  step: 4720  loss: 0.3388\n",
            "Epoch: 0  step: 4740  loss: 0.3483\n",
            "Epoch: 0  step: 4760  loss: 0.3601\n",
            "Epoch: 0  step: 4780  loss: 0.3410\n",
            "Epoch: 0  step: 4800  loss: 0.3576\n",
            "Epoch: 0  step: 4820  loss: 0.3576\n",
            "Epoch: 0  step: 4840  loss: 0.3813\n",
            "Epoch: 0  step: 4860  loss: 0.3462\n",
            "Epoch: 0  step: 4880  loss: 0.3402\n",
            "Epoch: 0  step: 4900  loss: 0.3466\n",
            "Epoch: 0  step: 4920  loss: 0.3556\n",
            "Epoch: 0  step: 4940  loss: 0.3537\n",
            "Epoch: 0  step: 4960  loss: 0.3495\n",
            "Epoch: 0  step: 4980  loss: 0.3441\n",
            "Epoch: 0  step: 5000  loss: 0.3359\n",
            "Epoch: 0  step: 5020  loss: 0.3445\n",
            "Epoch: 0  step: 5040  loss: 0.3519\n",
            "Epoch: 0  step: 5060  loss: 0.3348\n",
            "Epoch: 0  step: 5080  loss: 0.3387\n",
            "Epoch: 0  step: 5100  loss: 0.3296\n",
            "Epoch: 0  step: 5120  loss: 0.3339\n",
            "Epoch: 0  step: 5140  loss: 0.3446\n",
            "Epoch: 0  step: 5160  loss: 0.3367\n",
            "Epoch: 0  step: 5180  loss: 0.3305\n",
            "Epoch: 0  step: 5200  loss: 0.3363\n",
            "Epoch: 0  step: 5220  loss: 0.3364\n",
            "Epoch: 0  step: 5240  loss: 0.3402\n",
            "Epoch: 0  step: 5260  loss: 0.3390\n",
            "Epoch: 0  step: 5280  loss: 0.3287\n",
            "Epoch: 0  step: 5300  loss: 0.3323\n",
            "Epoch: 0  step: 5320  loss: 0.3219\n",
            "Epoch: 0  step: 5340  loss: 0.3524\n",
            "Epoch: 0  step: 5360  loss: 0.3426\n",
            "Epoch: 0  step: 5380  loss: 0.3309\n",
            "Epoch: 0  step: 5400  loss: 0.3283\n",
            "Epoch: 0  step: 5420  loss: 0.3298\n",
            "Epoch: 0  step: 5440  loss: 0.3298\n",
            "Epoch: 0  step: 5460  loss: 0.3222\n",
            "Epoch: 0  step: 5480  loss: 0.3267\n",
            "Epoch: 0  step: 5500  loss: 0.3338\n",
            "Epoch: 0  step: 5520  loss: 0.3352\n",
            "Epoch: 0  step: 5540  loss: 0.3303\n",
            "Epoch: 0  step: 5560  loss: 0.3291\n",
            "Epoch: 0  step: 5580  loss: 0.3426\n",
            "Epoch: 0  step: 5600  loss: 0.3163\n",
            "Epoch: 0  step: 5620  loss: 0.3285\n",
            "Epoch: 0  step: 5640  loss: 0.3346\n",
            "Epoch: 0  step: 5660  loss: 0.3246\n",
            "Epoch: 0  step: 5680  loss: 0.3315\n",
            "Epoch: 0  step: 5700  loss: 0.3325\n",
            "Epoch: 0  step: 5720  loss: 0.3207\n",
            "Epoch: 0  step: 5740  loss: 0.3311\n",
            "Epoch: 0  step: 5760  loss: 0.3236\n",
            "Epoch: 0  step: 5780  loss: 0.3271\n",
            "Epoch: 0  step: 5800  loss: 0.3178\n",
            "Epoch: 0  step: 5820  loss: 0.3235\n",
            "Epoch: 0  step: 5840  loss: 0.3148\n",
            "Epoch: 0  step: 5860  loss: 0.3344\n",
            "Epoch: 0  step: 5880  loss: 0.3234\n",
            "Epoch: 0  step: 5900  loss: 0.3202\n",
            "Epoch: 0  step: 5920  loss: 0.3189\n",
            "Epoch: 0  step: 5940  loss: 0.3280\n",
            "Epoch: 0  step: 5960  loss: 0.3182\n",
            "Epoch: 0  step: 5980  loss: 0.3175\n",
            "Epoch: 0  step: 6000  loss: 0.3184\n",
            "Epoch: 0  step: 6020  loss: 0.3068\n",
            "Epoch: 0  step: 6040  loss: 0.3172\n",
            "Epoch: 0  step: 6060  loss: 0.3189\n",
            "Epoch: 0  step: 6080  loss: 0.3309\n",
            "Epoch: 0  step: 6100  loss: 0.3181\n",
            "Epoch: 0  step: 6120  loss: 0.3217\n",
            "Epoch: 0  step: 6140  loss: 0.3154\n",
            "Epoch: 0  step: 6160  loss: 0.3210\n",
            "Epoch: 0  step: 6180  loss: 0.3078\n",
            "Epoch: 0  step: 6200  loss: 0.3137\n",
            "Epoch: 0  step: 6220  loss: 0.3154\n",
            "Epoch: 0  step: 6240  loss: 0.3099\n",
            "Epoch: 0  step: 6260  loss: 0.3191\n",
            "Epoch: 0  step: 6280  loss: 0.3076\n",
            "Epoch: 0  step: 6300  loss: 0.3103\n",
            "Epoch: 0  step: 6320  loss: 0.3153\n",
            "Epoch: 0  step: 6340  loss: 0.3066\n",
            "Epoch: 0  step: 6360  loss: 0.3087\n",
            "Epoch: 0  step: 6380  loss: 0.3091\n",
            "Epoch: 0  step: 6400  loss: 0.3128\n",
            "Epoch: 0  step: 6420  loss: 0.3081\n",
            "Epoch: 0  step: 6440  loss: 0.2996\n",
            "Epoch: 0  step: 6460  loss: 0.3054\n",
            "Epoch: 0  step: 6480  loss: 0.3131\n",
            "Epoch: 0  step: 6500  loss: 0.3159\n",
            "Epoch: 0  step: 6520  loss: 0.3215\n",
            "Epoch: 0  step: 6540  loss: 0.3052\n",
            "Epoch: 0  step: 6560  loss: 0.2961\n",
            "Epoch: 0  step: 6580  loss: 0.3028\n",
            "Epoch: 0  step: 6600  loss: 0.3059\n",
            "Epoch: 0  step: 6620  loss: 0.3101\n",
            "Epoch: 0  step: 6640  loss: 0.3223\n",
            "Epoch: 0  step: 6660  loss: 0.3104\n",
            "Epoch: 0  step: 6680  loss: 0.3012\n",
            "Epoch: 0  step: 6700  loss: 0.3048\n",
            "Epoch: 0  step: 6720  loss: 0.3016\n",
            "Epoch: 0  step: 6740  loss: 0.3105\n",
            "Epoch: 0  step: 6760  loss: 0.3154\n",
            "Epoch: 0  step: 6780  loss: 0.2980\n",
            "Epoch: 0  step: 6800  loss: 0.3106\n",
            "Epoch: 0  step: 6820  loss: 0.3014\n",
            "Epoch: 0  step: 6840  loss: 0.3089\n",
            "Epoch: 0  step: 6860  loss: 0.3058\n",
            "Epoch: 0  step: 6880  loss: 0.3095\n",
            "Epoch: 0  step: 6900  loss: 0.2977\n",
            "Epoch: 0  step: 6920  loss: 0.3176\n",
            "Epoch: 0  step: 6940  loss: 0.3020\n",
            "Epoch: 0  step: 6960  loss: 0.2873\n",
            "Epoch: 0  step: 6980  loss: 0.2955\n",
            "Epoch: 0  step: 7000  loss: 0.2983\n",
            "Epoch: 0  step: 7020  loss: 0.2853\n",
            "Epoch: 0  step: 7040  loss: 0.2990\n",
            "Epoch: 0  step: 7060  loss: 0.3023\n",
            "Epoch: 0  step: 7080  loss: 0.2968\n",
            "Epoch: 0  step: 7100  loss: 0.3085\n",
            "Epoch: 0  step: 7120  loss: 0.3080\n",
            "Epoch: 0  step: 7140  loss: 0.2972\n",
            "Epoch: 0  step: 7160  loss: 0.2994\n",
            "Epoch: 0  step: 7180  loss: 0.3043\n",
            "Epoch: 0  step: 7200  loss: 0.3026\n",
            "Epoch: 0  step: 7220  loss: 0.2853\n",
            "Epoch: 0  step: 7240  loss: 0.2894\n",
            "Epoch: 0  step: 7260  loss: 0.2884\n",
            "Epoch: 0  step: 7280  loss: 0.2973\n",
            "Epoch: 0  step: 7300  loss: 0.2890\n",
            "Epoch: 0  step: 7320  loss: 0.2885\n",
            "Epoch: 0  step: 7340  loss: 0.2778\n",
            "Epoch: 0  step: 7360  loss: 0.2948\n",
            "Epoch: 0  step: 7380  loss: 0.2853\n",
            "Epoch: 0  step: 7400  loss: 0.2985\n",
            "Epoch: 0  step: 7420  loss: 0.2882\n",
            "Epoch: 0  step: 7440  loss: 0.2830\n",
            "Epoch: 0  step: 7460  loss: 0.3001\n",
            "Epoch: 0  step: 7480  loss: 0.2895\n",
            "Epoch: 0  step: 7500  loss: 0.2825\n",
            "Epoch: 0  step: 7520  loss: 0.2830\n",
            "Epoch: 0  step: 7540  loss: 0.2922\n",
            "Epoch: 0  step: 7560  loss: 0.2926\n",
            "Epoch: 0  step: 7580  loss: 0.2832\n",
            "Epoch: 0  step: 7600  loss: 0.2820\n",
            "Epoch: 0  step: 7620  loss: 0.2921\n",
            "Epoch: 0  step: 7640  loss: 0.2858\n",
            "Epoch: 0  step: 7660  loss: 0.2823\n",
            "Epoch: 0  step: 7680  loss: 0.2822\n",
            "Epoch: 0  step: 7700  loss: 0.2798\n",
            "Epoch: 0  step: 7720  loss: 0.2810\n",
            "Epoch: 0  step: 7740  loss: 0.2822\n",
            "Epoch: 0  step: 7760  loss: 0.2788\n",
            "Epoch: 0  step: 7780  loss: 0.2901\n",
            "Epoch: 0  step: 7800  loss: 0.2693\n",
            "Epoch: 1  step: 7820  loss: 0.2772\n",
            "Epoch: 1  step: 7840  loss: 0.2872\n",
            "Epoch: 1  step: 7860  loss: 0.2729\n",
            "Epoch: 1  step: 7880  loss: 0.2719\n",
            "Epoch: 1  step: 7900  loss: 0.2740\n",
            "Epoch: 1  step: 7920  loss: 0.2691\n",
            "Epoch: 1  step: 7940  loss: 0.2826\n",
            "Epoch: 1  step: 7960  loss: 0.2701\n",
            "Epoch: 1  step: 7980  loss: 0.2871\n",
            "Epoch: 1  step: 8000  loss: 0.2908\n",
            "Epoch: 1  step: 8020  loss: 0.2616\n",
            "Epoch: 1  step: 8040  loss: 0.2774\n",
            "Epoch: 1  step: 8060  loss: 0.2687\n",
            "Epoch: 1  step: 8080  loss: 0.2700\n",
            "Epoch: 1  step: 8100  loss: 0.2713\n",
            "Epoch: 1  step: 8120  loss: 0.2723\n",
            "Epoch: 1  step: 8140  loss: 0.2737\n",
            "Epoch: 1  step: 8160  loss: 0.2772\n",
            "Epoch: 1  step: 8180  loss: 0.2743\n",
            "Epoch: 1  step: 8200  loss: 0.2742\n",
            "Epoch: 1  step: 8220  loss: 0.2691\n",
            "Epoch: 1  step: 8240  loss: 0.2725\n",
            "Epoch: 1  step: 8260  loss: 0.2762\n",
            "Epoch: 1  step: 8280  loss: 0.2727\n",
            "Epoch: 1  step: 8300  loss: 0.2574\n",
            "Epoch: 1  step: 8320  loss: 0.2851\n",
            "Epoch: 1  step: 8340  loss: 0.2667\n",
            "Epoch: 1  step: 8360  loss: 0.2736\n",
            "Epoch: 1  step: 8380  loss: 0.2658\n",
            "Epoch: 1  step: 8400  loss: 0.2626\n",
            "Epoch: 1  step: 8420  loss: 0.2635\n",
            "Epoch: 1  step: 8440  loss: 0.2658\n",
            "Epoch: 1  step: 8460  loss: 0.2697\n",
            "Epoch: 1  step: 8480  loss: 0.2557\n",
            "Epoch: 1  step: 8500  loss: 0.2813\n",
            "Epoch: 1  step: 8520  loss: 0.2617\n",
            "Epoch: 1  step: 8540  loss: 0.2670\n",
            "Epoch: 1  step: 8560  loss: 0.2873\n",
            "Epoch: 1  step: 8580  loss: 0.2708\n",
            "Epoch: 1  step: 8600  loss: 0.2658\n",
            "Epoch: 1  step: 8620  loss: 0.2702\n",
            "Epoch: 1  step: 8640  loss: 0.2614\n",
            "Epoch: 1  step: 8660  loss: 0.2743\n",
            "Epoch: 1  step: 8680  loss: 0.2759\n",
            "Epoch: 1  step: 8700  loss: 0.2663\n",
            "Epoch: 1  step: 8720  loss: 0.2758\n",
            "Epoch: 1  step: 8740  loss: 0.2623\n",
            "Epoch: 1  step: 8760  loss: 0.2537\n",
            "Epoch: 1  step: 8780  loss: 0.2561\n",
            "Epoch: 1  step: 8800  loss: 0.2677\n",
            "Epoch: 1  step: 8820  loss: 0.2636\n",
            "Epoch: 1  step: 8840  loss: 0.2623\n",
            "Epoch: 1  step: 8860  loss: 0.2574\n",
            "Epoch: 1  step: 8880  loss: 0.2718\n",
            "Epoch: 1  step: 8900  loss: 0.2544\n",
            "Epoch: 1  step: 8920  loss: 0.2608\n",
            "Epoch: 1  step: 8940  loss: 0.2570\n",
            "Epoch: 1  step: 8960  loss: 0.2665\n",
            "Epoch: 1  step: 8980  loss: 0.2554\n",
            "Epoch: 1  step: 9000  loss: 0.2521\n",
            "Epoch: 1  step: 9020  loss: 0.2682\n",
            "Epoch: 1  step: 9040  loss: 0.2569\n",
            "Epoch: 1  step: 9060  loss: 0.2591\n",
            "Epoch: 1  step: 9080  loss: 0.2612\n",
            "Epoch: 1  step: 9100  loss: 0.2740\n",
            "Epoch: 1  step: 9120  loss: 0.2575\n",
            "Epoch: 1  step: 9140  loss: 0.2586\n",
            "Epoch: 1  step: 9160  loss: 0.2735\n",
            "Epoch: 1  step: 9180  loss: 0.2527\n",
            "Epoch: 1  step: 9200  loss: 0.2557\n",
            "Epoch: 1  step: 9220  loss: 0.2668\n",
            "Epoch: 1  step: 9240  loss: 0.2589\n",
            "Epoch: 1  step: 9260  loss: 0.2621\n",
            "Epoch: 1  step: 9280  loss: 0.2769\n",
            "Epoch: 1  step: 9300  loss: 0.2551\n",
            "Epoch: 1  step: 9320  loss: 0.2516\n",
            "Epoch: 1  step: 9340  loss: 0.2624\n",
            "Epoch: 1  step: 9360  loss: 0.2601\n",
            "Epoch: 1  step: 9380  loss: 0.2661\n",
            "Epoch: 1  step: 9400  loss: 0.2452\n",
            "Epoch: 1  step: 9420  loss: 0.2560\n",
            "Epoch: 1  step: 9440  loss: 0.2688\n",
            "Epoch: 1  step: 9460  loss: 0.2656\n",
            "Epoch: 1  step: 9480  loss: 0.2609\n",
            "Epoch: 1  step: 9500  loss: 0.2443\n",
            "Epoch: 1  step: 9520  loss: 0.2522\n",
            "Epoch: 1  step: 9540  loss: 0.2693\n",
            "Epoch: 1  step: 9560  loss: 0.2528\n",
            "Epoch: 1  step: 9580  loss: 0.2524\n",
            "Epoch: 1  step: 9600  loss: 0.2464\n",
            "Epoch: 1  step: 9620  loss: 0.2544\n",
            "Epoch: 1  step: 9640  loss: 0.2576\n",
            "Epoch: 1  step: 9660  loss: 0.2493\n",
            "Epoch: 1  step: 9680  loss: 0.2488\n",
            "Epoch: 1  step: 9700  loss: 0.2741\n",
            "Epoch: 1  step: 9720  loss: 0.2580\n",
            "Epoch: 1  step: 9740  loss: 0.2505\n",
            "Epoch: 1  step: 9760  loss: 0.2662\n",
            "Epoch: 1  step: 9780  loss: 0.2495\n",
            "Epoch: 1  step: 9800  loss: 0.2486\n",
            "Epoch: 1  step: 9820  loss: 0.2597\n",
            "Epoch: 1  step: 9840  loss: 0.2543\n",
            "Epoch: 1  step: 9860  loss: 0.2485\n",
            "Epoch: 1  step: 9880  loss: 0.2642\n",
            "Epoch: 1  step: 9900  loss: 0.2486\n",
            "Epoch: 1  step: 9920  loss: 0.2460\n",
            "Epoch: 1  step: 9940  loss: 0.2581\n",
            "Epoch: 1  step: 9960  loss: 0.2415\n",
            "Epoch: 1  step: 9980  loss: 0.2685\n",
            "Epoch: 1  step: 10000  loss: 0.2631\n",
            "Epoch: 1  step: 10020  loss: 0.2474\n",
            "Epoch: 1  step: 10040  loss: 0.2585\n",
            "Epoch: 1  step: 10060  loss: 0.2515\n",
            "Epoch: 1  step: 10080  loss: 0.2540\n",
            "Epoch: 1  step: 10100  loss: 0.2645\n",
            "Epoch: 1  step: 10120  loss: 0.2473\n",
            "Epoch: 1  step: 10140  loss: 0.2442\n",
            "Epoch: 1  step: 10160  loss: 0.2469\n",
            "Epoch: 1  step: 10180  loss: 0.2493\n",
            "Epoch: 1  step: 10200  loss: 0.2551\n",
            "Epoch: 1  step: 10220  loss: 0.2540\n",
            "Epoch: 1  step: 10240  loss: 0.2482\n",
            "Epoch: 1  step: 10260  loss: 0.2450\n",
            "Epoch: 1  step: 10280  loss: 0.2563\n",
            "Epoch: 1  step: 10300  loss: 0.2458\n",
            "Epoch: 1  step: 10320  loss: 0.2492\n",
            "Epoch: 1  step: 10340  loss: 0.2482\n",
            "Epoch: 1  step: 10360  loss: 0.2571\n",
            "Epoch: 1  step: 10380  loss: 0.2678\n",
            "Epoch: 1  step: 10400  loss: 0.2617\n",
            "Epoch: 1  step: 10420  loss: 0.2400\n",
            "Epoch: 1  step: 10440  loss: 0.2518\n",
            "Epoch: 1  step: 10460  loss: 0.2550\n",
            "Epoch: 1  step: 10480  loss: 0.2439\n"
          ]
        }
      ],
      "source": [
        "! python model_train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJEkaZmVUvs3",
        "outputId": "ca01f4a5-6e44-4591-8a33-64cf0b78d728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normal samples: 1550712, Attack samples: 72406\n",
            "Final shapes -> Train: (1000000, 10)  Test: (623118, 10)\n",
            "threshold:  -4.205\n",
            "/usr/local/lib/python3.12/dist-packages/torch/functional.py:2066: UserWarning: torch.chain_matmul is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot instead, which accepts a list of two or more tensors rather than multiple parameters. (Triggered internally at /pytorch/aten/src/ATen/native/LinearAlgebra.cpp:1110.)\n",
            "  return _VF.chain_matmul(matrices)  # type: ignore[attr-defined]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/model_eval.py\", line 111, in <module>\n",
            "    main()\n",
            "  File \"/content/model_eval.py\", line 100, in main\n",
            "    sample_energy = estimator.sample_energy(m_prob, m_mean, m_cov, zi)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/dagmmv1.py\", line 339, in sample_energy\n",
            "    det_cov = torch.det(cov_k)\n",
            "              ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! python model_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0nJ0Ly_KXfQ",
        "outputId": "4eaa2fc9-a5e3-408b-81bb-4eccb2b2366d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   IPV4_SRC_ADDR  L4_SRC_PORT  IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  L7_PROTO  \\\n",
            "0  149.171.126.0        62073     59.166.0.5        56082         6       0.0   \n",
            "1  149.171.126.2        32284     59.166.0.5         1526         6       0.0   \n",
            "2  149.171.126.0           21     59.166.0.1        21971         6       1.0   \n",
            "3     59.166.0.1        23800  149.171.126.0        46893         6       0.0   \n",
            "4     59.166.0.5        63062  149.171.126.2           21         6       1.0   \n",
            "\n",
            "   IN_BYTES  OUT_BYTES  IN_PKTS  OUT_PKTS  TCP_FLAGS  \\\n",
            "0      9672        416       11         8         25   \n",
            "1      1776        104        6         2         25   \n",
            "2      1842       1236       26        22         25   \n",
            "3       528       8824       10        12         27   \n",
            "4      1786       2340       32        34         25   \n",
            "\n",
            "   FLOW_DURATION_MILLISECONDS  Label  Attack  \n",
            "0                          15      0  Benign  \n",
            "1                           0      0  Benign  \n",
            "2                        1111      0  Benign  \n",
            "3                         124      0  Benign  \n",
            "4                        1459      0  Benign  \n",
            "\n",
            "Data types of each column:\n",
            "IPV4_SRC_ADDR                  object\n",
            "L4_SRC_PORT                     int64\n",
            "IPV4_DST_ADDR                  object\n",
            "L4_DST_PORT                     int64\n",
            "PROTOCOL                        int64\n",
            "L7_PROTO                      float64\n",
            "IN_BYTES                        int64\n",
            "OUT_BYTES                       int64\n",
            "IN_PKTS                         int64\n",
            "OUT_PKTS                        int64\n",
            "TCP_FLAGS                       int64\n",
            "FLOW_DURATION_MILLISECONDS      int64\n",
            "Label                           int64\n",
            "Attack                         object\n",
            "dtype: object\n",
            "\n",
            "Summary of the dataframe:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1623118 entries, 0 to 1623117\n",
            "Data columns (total 14 columns):\n",
            " #   Column                      Non-Null Count    Dtype  \n",
            "---  ------                      --------------    -----  \n",
            " 0   IPV4_SRC_ADDR               1623118 non-null  object \n",
            " 1   L4_SRC_PORT                 1623118 non-null  int64  \n",
            " 2   IPV4_DST_ADDR               1623118 non-null  object \n",
            " 3   L4_DST_PORT                 1623118 non-null  int64  \n",
            " 4   PROTOCOL                    1623118 non-null  int64  \n",
            " 5   L7_PROTO                    1623118 non-null  float64\n",
            " 6   IN_BYTES                    1623118 non-null  int64  \n",
            " 7   OUT_BYTES                   1623118 non-null  int64  \n",
            " 8   IN_PKTS                     1623118 non-null  int64  \n",
            " 9   OUT_PKTS                    1623118 non-null  int64  \n",
            " 10  TCP_FLAGS                   1623118 non-null  int64  \n",
            " 11  FLOW_DURATION_MILLISECONDS  1623118 non-null  int64  \n",
            " 12  Label                       1623118 non-null  int64  \n",
            " 13  Attack                      1623118 non-null  object \n",
            "dtypes: float64(1), int64(10), object(3)\n",
            "memory usage: 173.4+ MB\n",
            "None\n",
            "       IPV4_SRC_ADDR   L4_SRC_PORT  IPV4_DST_ADDR   L4_DST_PORT      PROTOCOL  \\\n",
            "count        1623118  1.623118e+06        1623118  1.623118e+06  1.623118e+06   \n",
            "unique            40           NaN             40           NaN           NaN   \n",
            "top       59.166.0.5           NaN  149.171.126.1           NaN           NaN   \n",
            "freq          153762           NaN         153450           NaN           NaN   \n",
            "mean             NaN  3.264150e+04            NaN  1.381955e+04  9.890804e+00   \n",
            "std              NaN  1.919783e+04            NaN  1.959370e+04  1.252321e+01   \n",
            "min              NaN  0.000000e+00            NaN  0.000000e+00  0.000000e+00   \n",
            "25%              NaN  1.591600e+04            NaN  5.300000e+01  6.000000e+00   \n",
            "50%              NaN  3.267500e+04            NaN  2.627000e+03  6.000000e+00   \n",
            "75%              NaN  4.934800e+04            NaN  2.455900e+04  1.700000e+01   \n",
            "max              NaN  6.553500e+04            NaN  6.553500e+04  2.550000e+02   \n",
            "\n",
            "            L7_PROTO      IN_BYTES     OUT_BYTES       IN_PKTS      OUT_PKTS  \\\n",
            "count   1.623118e+06  1.623118e+06  1.623118e+06  1.623118e+06  1.623118e+06   \n",
            "unique           NaN           NaN           NaN           NaN           NaN   \n",
            "top              NaN           NaN           NaN           NaN           NaN   \n",
            "freq             NaN           NaN           NaN           NaN           NaN   \n",
            "mean    1.365581e+01  5.336064e+03  4.273129e+04  3.953901e+01  5.078661e+01   \n",
            "std     2.062080e+01  7.841119e+04  1.754205e+05  8.883628e+01  1.315082e+02   \n",
            "min     0.000000e+00  5.200000e+01  0.000000e+00  1.000000e+00  0.000000e+00   \n",
            "25%     0.000000e+00  5.200000e+02  3.040000e+02  4.000000e+00  4.000000e+00   \n",
            "50%     5.000000e+00  1.920000e+03  3.276000e+03  1.600000e+01  1.800000e+01   \n",
            "75%     1.300000e+01  3.806000e+03  2.100600e+04  5.000000e+01  4.800000e+01   \n",
            "max     2.490000e+02  2.685425e+07  1.465675e+07  2.003800e+04  1.102400e+04   \n",
            "\n",
            "           TCP_FLAGS  FLOW_DURATION_MILLISECONDS         Label   Attack  \n",
            "count   1.623118e+06                1.623118e+06  1.623118e+06  1623118  \n",
            "unique           NaN                         NaN           NaN       10  \n",
            "top              NaN                         NaN           NaN   Benign  \n",
            "freq             NaN                         NaN           NaN  1550712  \n",
            "mean    1.906669e+01                5.515161e+04  4.460920e-02      NaN  \n",
            "std     1.206588e+01                4.811634e+05  2.064443e-01      NaN  \n",
            "min     0.000000e+00                0.000000e+00  0.000000e+00      NaN  \n",
            "25%     0.000000e+00                0.000000e+00  0.000000e+00      NaN  \n",
            "50%     2.700000e+01                7.000000e+00  0.000000e+00      NaN  \n",
            "75%     2.700000e+01                1.660000e+02  0.000000e+00      NaN  \n",
            "max     3.100000e+01                4.294952e+06  1.000000e+00      NaN  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your csv\n",
        "df = pd.read_csv(\"NF-UNSW-NB15.csv\")\n",
        "\n",
        "# See first few rows\n",
        "print(df.head())\n",
        "\n",
        "# Column names and datatypes\n",
        "print(\"\\nData types of each column:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Quick summary (min, max, mean for numeric; counts for categorical)\n",
        "print(\"\\nSummary of the dataframe:\")\n",
        "print(df.info())\n",
        "print(df.describe(include=\"all\"))  # include='all' shows categorical too\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnvAVUBiHtCx",
        "outputId": "d592f02f-a4ba-4ecc-aae1-312532b13d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys inside the npz file: ['kdd']\n",
            "\n",
            "Key: kdd\n",
            "Shape: (494021, 119)\n",
            "Dtype: float64\n",
            "First few values:\n",
            " [[0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 2.61041764e-07 1.05713002e-03 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.56555773e-02\n",
            "  1.56555773e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  3.14960630e-02 3.14960630e-02 1.00000000e+00 0.00000000e+00\n",
            "  1.10000000e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00]\n",
            " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.44690506e-07 9.42688423e-05 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.56555773e-02\n",
            "  1.56555773e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  7.08661417e-02 7.08661417e-02 1.00000000e+00 0.00000000e+00\n",
            "  5.00000000e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00]]\n",
            "(1, 119)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the npz file\n",
        "data = np.load(\"/content/kdd_cup.npz\")\n",
        "\n",
        "# List all arrays stored inside\n",
        "print(\"Keys inside the npz file:\", data.files)\n",
        "\n",
        "# Inspect each array\n",
        "for key in data.files:\n",
        "    print(f\"\\nKey: {key}\")\n",
        "    print(\"Shape:\", data[key].shape)\n",
        "    print(\"Dtype:\", data[key].dtype)\n",
        "    print(\"First few values:\\n\", data[key][:2])\n",
        "    print(data[key][:1].shape)  # show first 10 elements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Vvtv8b6b1Y",
        "outputId": "905cdf48-139e-48e3-8a03-58d1d0c4e561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problematic columns: [('IPV4_SRC_ADDR', 'Non-numeric'), ('IPV4_DST_ADDR', 'Non-numeric'), ('SRC_TO_DST_SECOND_BYTES', 'Contains NaN'), ('SRC_TO_DST_SECOND_BYTES', 'Contains inf'), ('DST_TO_SRC_SECOND_BYTES', 'Contains inf'), ('Attack', 'Non-numeric')]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def check_problematic_columns(df):\n",
        "    problematic_cols = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Check for non-numeric columns\n",
        "        if not np.issubdtype(df[col].dtype, np.number):\n",
        "            problematic_cols.append((col, 'Non-numeric'))\n",
        "            continue\n",
        "\n",
        "        # Check for NaNs\n",
        "        if df[col].isna().any():\n",
        "            problematic_cols.append((col, 'Contains NaN'))\n",
        "\n",
        "        # Check for inf/-inf\n",
        "        if np.isinf(df[col].values).any():\n",
        "            problematic_cols.append((col, 'Contains inf'))\n",
        "\n",
        "        # Optional: check for zero variance (might break some models)\n",
        "        if df[col].nunique() <= 1:\n",
        "            problematic_cols.append((col, 'Zero variance'))\n",
        "\n",
        "    return problematic_cols\n",
        "\n",
        "\n",
        "# Example usage\n",
        "df = pd.read_csv(\"/content/NF-UNSW-NB15-v3.csv\")  # your dataset\n",
        "# After your MinMaxScaler preprocessing\n",
        "problem_cols = check_problematic_columns(df)\n",
        "print(\"Problematic columns:\", problem_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx4A4zylxikD",
        "outputId": "896628cb-d42c-4e6e-beac-37b3847cc1b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing paths_1.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile paths_1.py\n",
        "print(\"Data_nb15.py!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz-g3Vqs3Mwq",
        "outputId": "5d0eb0e3-feb3-44a8-e691-518ffe382d33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned CSV saved as UNSW_NB15_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "\n",
        "# --------------------------\n",
        "# Configuration\n",
        "# --------------------------\n",
        " # original dataset\n",
        "output_filename = \"UNSW_NB15_cleaned.csv\"\n",
        "data_dir=\"/content/\"\n",
        "\n",
        "# --------------------------\n",
        "# Load dataset\n",
        "# --------------------------\n",
        "df = pd.read_csv(\"/content/NF-UNSW-NB15-v3.csv\")\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ Convert IP addresses to integers\n",
        "# --------------------------\n",
        "def ip_to_int(ip):\n",
        "    try:\n",
        "        parts = list(map(int, str(ip).split('.')))\n",
        "        return parts[0]*256**3 + parts[1]*256**2 + parts[2]*256 + parts[3]\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "for col in ['IPV4_SRC_ADDR', 'IPV4_DST_ADDR']:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].apply(ip_to_int)\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ Encode categorical features\n",
        "# --------------------------\n",
        "# categorical_cols = ['proto', 'service']  # add more if needed\n",
        "# for col in categorical_cols:\n",
        "#     if col in df.columns:\n",
        "#         le = LabelEncoder()\n",
        "#         df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "# --------------------------\n",
        "# 3️⃣ Handle NaN and inf\n",
        "# --------------------------\n",
        "# Replace inf/-inf with NaN\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Impute numeric columns with median\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "imp = SimpleImputer(strategy='median')\n",
        "df[numeric_cols] = imp.fit_transform(df[numeric_cols])\n",
        "\n",
        "# --------------------------\n",
        "# 4️⃣ Convert class label to numeric\n",
        "# --------------------------\n",
        "# if 'Attack' in df.columns:\n",
        "#     le = LabelEncoder()\n",
        "#     df['Attack'] = le.fit_transform(df['Attack'].astype(str))\n",
        "\n",
        "# --------------------------\n",
        "# 5️⃣ Scale numeric features (excluding label)\n",
        "# --------------------------\n",
        "features = df.drop(columns=['Attack'])\n",
        "scaler = MinMaxScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "df_scaled = pd.DataFrame(features_scaled, columns=features.columns)\n",
        "df_scaled['Attack'] = df['Attack'].values\n",
        "\n",
        "# --------------------------\n",
        "# 6️⃣ Save cleaned CSV\n",
        "# --------------------------\n",
        "df_scaled.to_csv(os.path.join(data_dir, output_filename), index=False)\n",
        "print(f\"Cleaned CSV saved as {output_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Lfxv0jvV_7sF",
        "outputId": "7ee7cea4-ed08-49f3-cd54-0fd83216bacd"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_2544b5cf-f55c-4b7b-a262-3e93283b5479\", \"UNSW_NB15_cleaned.csv\", 2051622307)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Make sure the cleaned CSV is saved first\n",
        "df_scaled.to_csv(\"UNSW_NB15_cleaned.csv\", index=False)\n",
        "\n",
        "# Download to your PC\n",
        "files.download(\"UNSW_NB15_cleaned.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQgmFSd2nVu4",
        "outputId": "1f3a6a97-1cdb-4758-dbcb-8d129be807bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: (247540, 55)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the cleaned CSV\n",
        "df = pd.read_csv(\"UNSW_NB15_cleaned.csv\")\n",
        "\n",
        "# 1️⃣ Quick overview\n",
        "print(\"Shape of dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj-fEeWUBBNv",
        "outputId": "f4b76714-c956-4d62-ca84-5dcafb6cc102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of dataset: (651286, 55)\n",
            "Columns: ['FLOW_START_MILLISECONDS', 'FLOW_END_MILLISECONDS', 'IPV4_SRC_ADDR', 'L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT', 'PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'IN_PKTS', 'OUT_BYTES', 'OUT_PKTS', 'TCP_FLAGS', 'CLIENT_TCP_FLAGS', 'SERVER_TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'DURATION_IN', 'DURATION_OUT', 'MIN_TTL', 'MAX_TTL', 'LONGEST_FLOW_PKT', 'SHORTEST_FLOW_PKT', 'MIN_IP_PKT_LEN', 'MAX_IP_PKT_LEN', 'SRC_TO_DST_SECOND_BYTES', 'DST_TO_SRC_SECOND_BYTES', 'RETRANSMITTED_IN_BYTES', 'RETRANSMITTED_IN_PKTS', 'RETRANSMITTED_OUT_BYTES', 'RETRANSMITTED_OUT_PKTS', 'SRC_TO_DST_AVG_THROUGHPUT', 'DST_TO_SRC_AVG_THROUGHPUT', 'NUM_PKTS_UP_TO_128_BYTES', 'NUM_PKTS_128_TO_256_BYTES', 'NUM_PKTS_256_TO_512_BYTES', 'NUM_PKTS_512_TO_1024_BYTES', 'NUM_PKTS_1024_TO_1514_BYTES', 'TCP_WIN_MAX_IN', 'TCP_WIN_MAX_OUT', 'ICMP_TYPE', 'ICMP_IPV4_TYPE', 'DNS_QUERY_ID', 'DNS_QUERY_TYPE', 'DNS_TTL_ANSWER', 'FTP_COMMAND_RET_CODE', 'SRC_TO_DST_IAT_MIN', 'SRC_TO_DST_IAT_MAX', 'SRC_TO_DST_IAT_AVG', 'SRC_TO_DST_IAT_STDDEV', 'DST_TO_SRC_IAT_MIN', 'DST_TO_SRC_IAT_MAX', 'DST_TO_SRC_IAT_AVG', 'DST_TO_SRC_IAT_STDDEV', 'Label', 'Attack']\n",
            "\n",
            "Data types:\n",
            "FLOW_START_MILLISECONDS        float64\n",
            "FLOW_END_MILLISECONDS          float64\n",
            "IPV4_SRC_ADDR                  float64\n",
            "L4_SRC_PORT                    float64\n",
            "IPV4_DST_ADDR                  float64\n",
            "L4_DST_PORT                    float64\n",
            "PROTOCOL                       float64\n",
            "L7_PROTO                       float64\n",
            "IN_BYTES                       float64\n",
            "IN_PKTS                        float64\n",
            "OUT_BYTES                      float64\n",
            "OUT_PKTS                       float64\n",
            "TCP_FLAGS                      float64\n",
            "CLIENT_TCP_FLAGS               float64\n",
            "SERVER_TCP_FLAGS               float64\n",
            "FLOW_DURATION_MILLISECONDS     float64\n",
            "DURATION_IN                    float64\n",
            "DURATION_OUT                   float64\n",
            "MIN_TTL                        float64\n",
            "MAX_TTL                        float64\n",
            "LONGEST_FLOW_PKT               float64\n",
            "SHORTEST_FLOW_PKT              float64\n",
            "MIN_IP_PKT_LEN                 float64\n",
            "MAX_IP_PKT_LEN                 float64\n",
            "SRC_TO_DST_SECOND_BYTES        float64\n",
            "DST_TO_SRC_SECOND_BYTES        float64\n",
            "RETRANSMITTED_IN_BYTES         float64\n",
            "RETRANSMITTED_IN_PKTS          float64\n",
            "RETRANSMITTED_OUT_BYTES        float64\n",
            "RETRANSMITTED_OUT_PKTS         float64\n",
            "SRC_TO_DST_AVG_THROUGHPUT      float64\n",
            "DST_TO_SRC_AVG_THROUGHPUT      float64\n",
            "NUM_PKTS_UP_TO_128_BYTES       float64\n",
            "NUM_PKTS_128_TO_256_BYTES      float64\n",
            "NUM_PKTS_256_TO_512_BYTES      float64\n",
            "NUM_PKTS_512_TO_1024_BYTES     float64\n",
            "NUM_PKTS_1024_TO_1514_BYTES    float64\n",
            "TCP_WIN_MAX_IN                 float64\n",
            "TCP_WIN_MAX_OUT                float64\n",
            "ICMP_TYPE                      float64\n",
            "ICMP_IPV4_TYPE                 float64\n",
            "DNS_QUERY_ID                   float64\n",
            "DNS_QUERY_TYPE                 float64\n",
            "DNS_TTL_ANSWER                 float64\n",
            "FTP_COMMAND_RET_CODE           float64\n",
            "SRC_TO_DST_IAT_MIN             float64\n",
            "SRC_TO_DST_IAT_MAX             float64\n",
            "SRC_TO_DST_IAT_AVG             float64\n",
            "SRC_TO_DST_IAT_STDDEV          float64\n",
            "DST_TO_SRC_IAT_MIN             float64\n",
            "DST_TO_SRC_IAT_MAX             float64\n",
            "DST_TO_SRC_IAT_AVG             float64\n",
            "DST_TO_SRC_IAT_STDDEV          float64\n",
            "Label                          float64\n",
            "Attack                          object\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "FLOW_START_MILLISECONDS        0\n",
            "FLOW_END_MILLISECONDS          0\n",
            "IPV4_SRC_ADDR                  0\n",
            "L4_SRC_PORT                    0\n",
            "IPV4_DST_ADDR                  0\n",
            "L4_DST_PORT                    0\n",
            "PROTOCOL                       0\n",
            "L7_PROTO                       0\n",
            "IN_BYTES                       0\n",
            "IN_PKTS                        0\n",
            "OUT_BYTES                      0\n",
            "OUT_PKTS                       0\n",
            "TCP_FLAGS                      0\n",
            "CLIENT_TCP_FLAGS               0\n",
            "SERVER_TCP_FLAGS               0\n",
            "FLOW_DURATION_MILLISECONDS     0\n",
            "DURATION_IN                    0\n",
            "DURATION_OUT                   0\n",
            "MIN_TTL                        0\n",
            "MAX_TTL                        0\n",
            "LONGEST_FLOW_PKT               0\n",
            "SHORTEST_FLOW_PKT              0\n",
            "MIN_IP_PKT_LEN                 0\n",
            "MAX_IP_PKT_LEN                 0\n",
            "SRC_TO_DST_SECOND_BYTES        0\n",
            "DST_TO_SRC_SECOND_BYTES        0\n",
            "RETRANSMITTED_IN_BYTES         0\n",
            "RETRANSMITTED_IN_PKTS          0\n",
            "RETRANSMITTED_OUT_BYTES        0\n",
            "RETRANSMITTED_OUT_PKTS         0\n",
            "SRC_TO_DST_AVG_THROUGHPUT      0\n",
            "DST_TO_SRC_AVG_THROUGHPUT      0\n",
            "NUM_PKTS_UP_TO_128_BYTES       0\n",
            "NUM_PKTS_128_TO_256_BYTES      0\n",
            "NUM_PKTS_256_TO_512_BYTES      0\n",
            "NUM_PKTS_512_TO_1024_BYTES     0\n",
            "NUM_PKTS_1024_TO_1514_BYTES    0\n",
            "TCP_WIN_MAX_IN                 0\n",
            "TCP_WIN_MAX_OUT                0\n",
            "ICMP_TYPE                      0\n",
            "ICMP_IPV4_TYPE                 0\n",
            "DNS_QUERY_ID                   1\n",
            "DNS_QUERY_TYPE                 1\n",
            "DNS_TTL_ANSWER                 1\n",
            "FTP_COMMAND_RET_CODE           1\n",
            "SRC_TO_DST_IAT_MIN             1\n",
            "SRC_TO_DST_IAT_MAX             1\n",
            "SRC_TO_DST_IAT_AVG             1\n",
            "SRC_TO_DST_IAT_STDDEV          1\n",
            "DST_TO_SRC_IAT_MIN             1\n",
            "DST_TO_SRC_IAT_MAX             1\n",
            "DST_TO_SRC_IAT_AVG             1\n",
            "DST_TO_SRC_IAT_STDDEV          1\n",
            "Label                          1\n",
            "Attack                         1\n",
            "dtype: int64\n",
            "\n",
            "Columns with inf/-inf:\n",
            "\n",
            "Descriptive statistics:\n",
            "       FLOW_START_MILLISECONDS  FLOW_END_MILLISECONDS  IPV4_SRC_ADDR  \\\n",
            "count            651286.000000          651286.000000  651286.000000   \n",
            "mean                  0.990679               0.990679       0.338546   \n",
            "std                   0.005267               0.005267       0.192263   \n",
            "min                   0.981347               0.981347       0.000000   \n",
            "25%                   0.986093               0.986093       0.271179   \n",
            "50%                   0.990416               0.990416       0.271179   \n",
            "75%                   0.995258               0.995258       0.271179   \n",
            "max                   1.000000               1.000000       0.904212   \n",
            "\n",
            "         L4_SRC_PORT  IPV4_DST_ADDR    L4_DST_PORT       PROTOCOL  \\\n",
            "count  651286.000000  651286.000000  651286.000000  651286.000000   \n",
            "mean        0.497927       0.654824       0.162550       0.034755   \n",
            "std         0.294340       0.021713       0.277147       0.026819   \n",
            "min         0.000000       0.000000       0.000000       0.000000   \n",
            "25%         0.240055       0.652405       0.000381       0.023529   \n",
            "50%         0.502296       0.652405       0.001221       0.023529   \n",
            "75%         0.748653       0.652405       0.190158       0.023529   \n",
            "max         1.000000       1.000000       1.000000       1.000000   \n",
            "\n",
            "            L7_PROTO       IN_BYTES        IN_PKTS  ...  FTP_COMMAND_RET_CODE  \\\n",
            "count  651286.000000  651286.000000  651286.000000  ...         651285.000000   \n",
            "mean        0.055578       0.000178       0.001676  ...              0.076950   \n",
            "std         0.054825       0.003297       0.004466  ...              0.170452   \n",
            "min         0.000000       0.000000       0.000000  ...              0.000000   \n",
            "25%         0.011876       0.000016       0.000149  ...              0.000000   \n",
            "50%         0.085511       0.000058       0.000744  ...              0.000000   \n",
            "75%         0.087886       0.000110       0.002083  ...              0.000000   \n",
            "max         0.988124       1.000000       1.000000  ...              1.000000   \n",
            "\n",
            "       SRC_TO_DST_IAT_MIN  SRC_TO_DST_IAT_MAX  SRC_TO_DST_IAT_AVG  \\\n",
            "count       651285.000000       651285.000000       651285.000000   \n",
            "mean             0.000098            0.003884            0.000919   \n",
            "std              0.004274            0.015253            0.009103   \n",
            "min              0.000000            0.000000            0.000000   \n",
            "25%              0.000000            0.000017            0.000000   \n",
            "50%              0.000000            0.000099            0.000000   \n",
            "75%              0.000000            0.002871            0.000528   \n",
            "max              0.755272            0.950411            0.755272   \n",
            "\n",
            "       SRC_TO_DST_IAT_STDDEV  DST_TO_SRC_IAT_MIN  DST_TO_SRC_IAT_MAX  \\\n",
            "count          651285.000000        6.512850e+05       651285.000000   \n",
            "mean                0.001651        4.462010e-07            0.003731   \n",
            "std                 0.007440        2.063620e-04            0.013307   \n",
            "min                 0.000000        0.000000e+00            0.000000   \n",
            "25%                 0.000000        0.000000e+00            0.000017   \n",
            "50%                 0.000033        0.000000e+00            0.000099   \n",
            "75%                 0.001353        0.000000e+00            0.002888   \n",
            "max                 0.896036        1.550336e-01            0.935229   \n",
            "\n",
            "       DST_TO_SRC_IAT_AVG  DST_TO_SRC_IAT_STDDEV          Label  \n",
            "count       651285.000000          651285.000000  651285.000000  \n",
            "mean             0.000960               0.001572       0.091174  \n",
            "std              0.003280               0.004724       0.287856  \n",
            "min              0.000000               0.000000       0.000000  \n",
            "25%              0.000000               0.000000       0.000000  \n",
            "50%              0.000000               0.000035       0.000000  \n",
            "75%              0.000792               0.001540       0.000000  \n",
            "max              0.935244               0.935203       1.000000  \n",
            "\n",
            "[8 rows x 54 columns]\n",
            "\n",
            "First 5 rows:\n",
            "   FLOW_START_MILLISECONDS  FLOW_END_MILLISECONDS  IPV4_SRC_ADDR  L4_SRC_PORT  \\\n",
            "0                 0.991276               0.991276       0.271179     0.074678   \n",
            "1                 0.991276               0.991276       0.271179     0.803708   \n",
            "2                 0.991275               0.991276       0.271179     0.721599   \n",
            "3                 0.991276               0.991276       0.271179     0.660868   \n",
            "4                 0.991276               0.991276       0.271179     0.699931   \n",
            "\n",
            "   IPV4_DST_ADDR  L4_DST_PORT  PROTOCOL  L7_PROTO  IN_BYTES   IN_PKTS  ...  \\\n",
            "0       0.652405     0.000809  0.066667  0.011876  0.000005  0.000050  ...   \n",
            "1       0.652405     0.488167  0.023529  0.026128  0.000174  0.001339  ...   \n",
            "2       0.652405     0.104997  0.023529  0.087886  0.000506  0.011753  ...   \n",
            "3       0.652405     0.000809  0.066667  0.011876  0.000005  0.000050  ...   \n",
            "4       0.652405     0.000809  0.066667  0.011876  0.000005  0.000050  ...   \n",
            "\n",
            "   SRC_TO_DST_IAT_MIN  SRC_TO_DST_IAT_MAX  SRC_TO_DST_IAT_AVG  \\\n",
            "0                 0.0            0.000000            0.000000   \n",
            "1                 0.0            0.001502            0.000396   \n",
            "2                 0.0            0.030414            0.000330   \n",
            "3                 0.0            0.000000            0.000000   \n",
            "4                 0.0            0.000000            0.000000   \n",
            "\n",
            "   SRC_TO_DST_IAT_STDDEV  DST_TO_SRC_IAT_MIN  DST_TO_SRC_IAT_MAX  \\\n",
            "0               0.000000                 0.0            0.000000   \n",
            "1               0.000627                 0.0            0.001485   \n",
            "2               0.003928                 0.0            0.030414   \n",
            "3               0.000000                 0.0            0.000000   \n",
            "4               0.000000                 0.0            0.000000   \n",
            "\n",
            "   DST_TO_SRC_IAT_AVG  DST_TO_SRC_IAT_STDDEV  Label  Attack  \n",
            "0            0.000000               0.000000    0.0  Benign  \n",
            "1            0.000594               0.000665    0.0  Benign  \n",
            "2            0.000248               0.003081    0.0  Benign  \n",
            "3            0.000000               0.000000    0.0  Benign  \n",
            "4            0.000000               0.000000    0.0  Benign  \n",
            "\n",
            "[5 rows x 55 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Load the cleaned CSV\n",
        "df = pd.read_csv(\"/content/UNSW_NB15_cleaned.csv\")\n",
        "\n",
        "# 1️⃣ Quick overview\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# 2️⃣ Data types\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# 3️⃣ Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isna().sum())\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "print(\"\\nColumns with inf/-inf:\")\n",
        "for col in numeric_cols:\n",
        "    if np.isinf(df[col].values).any():\n",
        "        print(col)\n",
        "\n",
        "\n",
        "# 5️⃣ Basic statistics\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# 6️⃣ Peek at the first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}